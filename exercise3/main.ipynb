{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can omit this cell if you already have pkl files\n",
    "from feature_extraction import Method\n",
    "from readers import GetProcessedInputData\n",
    "from ipynb.fs.full.accuracy_metrics import CalculateAccuracy\n",
    "\n",
    "feature_extraction_methods = \\\n",
    "  [Method.BLACK_PIXEL_RATIO, Method.BLACK_PIXEL_RATIO_LC_UP, Method.LOWER_CONTOUR, Method.UPPER_CONTOUR]\n",
    "transcriptions_as_list, keywords_to_search, train_words, test_words = GetProcessedInputData(feature_extraction_methods)\n",
    "\n",
    "#Run line below if you would like to create dumpfiles\n",
    "#dumpToFile(inputDataVariableNames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "inputDataVariableNames = [\"transcriptions_as_list\", \"keywords_to_search\", \"train_words\", \"test_words\"]\n",
    "def dumpToFile(names):\n",
    "    if isinstance(names, str):\n",
    "        names= [names]\n",
    "    for name in names:\n",
    "        dumpFile = os.fsencode(os.getcwd() + \"/output/\" + name + \".pkl\")\n",
    "        with open(dumpFile,\"wb\") as f:\n",
    "            pickle.dump(eval(name), f)\n",
    "def dumpFromFile(names):\n",
    "    if isinstance(names, str):\n",
    "        names= [names]\n",
    "    for name in names:\n",
    "        print(name)\n",
    "        dumpedFile = os.fsencode(os.getcwd() + \"/output/\" + name + \".pkl\")\n",
    "        with open(dumpedFile,\"rb\") as f:\n",
    "            globals()[name] = pickle.load(f)\n",
    "            \n",
    "def pickleToCsv(filenames):\n",
    "    if isinstance(filenames, str):\n",
    "        filenames= [filenames]\n",
    "    for name in filenames:\n",
    "        with open(os.fsencode(os.getcwd() + \"/output/\" + name + \".pkl\"), \"rb\") as f:\n",
    "            l = pickle.load(f)\n",
    "            df = pd.DataFrame(l)\n",
    "            df.to_csv('output/{0}.csv'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcriptions_as_list\n",
      "keywords_to_search\n",
      "train_words\n",
      "test_words\n"
     ]
    }
   ],
   "source": [
    "dumpFromFile(inputDataVariableNames)\n",
    "pickleToCsv(inputDataVariableNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: For testing only. Remove the entire cell later\n",
    "\n",
    "# all_train_words = train_words\n",
    "# all_test_words = test_words\n",
    "\n",
    "train_words = train_words[:20]\n",
    "test_words = test_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dtaidistance import dtw\n",
    "from compute_distances import ComputeDistances\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "distances = ComputeDistances(train_words, test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle   \n",
    "#Put distances dictionary into a pickle file\n",
    "#Put the name of you feature\n",
    "file = open(\"output/FOUR_FEATURES.pkl\", \"wb\")\n",
    "pickle.dump(distances, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle   \n",
    "#Retrieve the distance dictionary from a pickle file\n",
    "#Put the name of you feature\n",
    "file_to_read = open(\"output/FOUR_FEATURES.pkl\", \"rb\")\n",
    "distances_load = pickle.load(file_to_read)\n",
    "file_to_read.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(distances_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyword_spotting import FindClosestKnownWord, SpotKeyword\n",
    "\n",
    "\n",
    "# For each test_word, find the train_word that is closest to it.\n",
    "for test_word in test_words:\n",
    "  test_word.closest_train_word, test_word.distance_to_closest_train_word = \\\n",
    "    FindClosestKnownWord(test_word, train_words, distances)\n",
    "    \n",
    "\n",
    "# Search keywords and save spotted keywords in the list.\n",
    "outputData = []\n",
    "for keyword_to_search in keywords_to_search:\n",
    "  spotted_test_keywords = SpotKeyword(keyword_to_search, test_words)\n",
    "  if len(spotted_test_keywords):\n",
    "    print(keyword_to_search)\n",
    "    print(spotted_test_keywords)\n",
    "    spottedKeyword = []\n",
    "    for keyword in spotted_test_keywords:\n",
    "      spottedKeyword.append(keyword.id)\n",
    "      spottedKeyword.append(keyword.distance_to_closest_train_word)\n",
    "      print(keyword.distance_to_closest_train_word)\n",
    "      keyword.image.show()\n",
    "    outputData.append([keyword_to_search, *spottedKeyword])\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write output file kws.csv (slide 24)\n",
    "import csv\n",
    "with open(\"kws.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(outputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics - In progress, not finished yet (There might be errors in the logic)\n",
    "# convert transcriptions to dict - easier and faster access values later\n",
    "from ipynb.fs.full.accuracy_metrics import CalculateAccuracy\n",
    "groundTruth = dict()\n",
    "for sub in transcriptions_as_list:\n",
    "    groundTruth[sub[0]] = sub[1]\n",
    "\n",
    "testWords = dict()\n",
    "for word in test_words:\n",
    "    testWords[word.id] = word.transcription\n",
    "\n",
    "predictions = dict()\n",
    "for item in outputData:\n",
    "    predictions[item[0]] = item[1::2]\n",
    "\n",
    "acc = CalculateAccuracy(keywords_to_search, testWords, predictions, groundTruth)\n",
    "\n",
    "\n",
    "print(acc.CalculateRecall())\n",
    "print(acc.CalculatePrecision())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
