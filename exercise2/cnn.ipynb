{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorials used:\n",
    "# - https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "# - https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_TEST = 1000 # I don't think this actually matters\n",
    "\n",
    "NUMBER_OF_EPOCHS = 25\n",
    "LEARNING_RATE = 0.05\n",
    "MOMENTUM = 0\n",
    "\n",
    "USE_PNG_FORMAT = False\n",
    "\n",
    "# The accuracy is measured on the test data, not the train one\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.01 and BATCH_SIZE=32 I got 76% accuracy after 25 EPOCHS\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.02 and BATCH_SIZE=32 I got 68% accuracy after 9 EPOCHS, 85% accuracy after 25 EPOCHS\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.05 and BATCH_SIZE=32 I got 78% accuracy after 25 EPOCHS\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.1  and BATCH_SIZE=32 I got 70% accuracy after 25 EPOCHS\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.05 and BATCH_SIZE=32 I got 88% accuracy after 25 EPOCHS, with Dropout(p=0.2) at the end of conv1 and conv2, 89 after 29 EPOCHS\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.05 and BATCH_SIZE=32 I got 88% accuracy after 11 EPOCHS, with Dropout(p=0.5) at the beginning of conv1 + Dropout(p=0.2) aththe end of conv2, LeakyReLU from conv2 removed\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.05 and BATCH_SIZE=32 I got 88% accuracy after 9 EPOCHS, with Dropout(p=0.2) at the beginning of conv1 + Dropout(p=0.2) aththe end of conv2, LeakyReLU from conv2 removed\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.05 and BATCH_SIZE=32 I got _% accuracy after _ EPOCHS, with Dropout2D(p=0.2) at the beginning of conv1 + Dropout2D(p=0.2) aththe end of conv2, LeakyReLU from conv2 removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exercise2_config as config\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(round(datetime.timestamp(datetime.now())))\n",
    "\n",
    "\n",
    "def ReadImages(dir):\n",
    "  images = []\n",
    "  for filename in os.listdir(dir):\n",
    "    image_path = os.path.join(dir, filename)\n",
    "    image = Image.open(image_path)\n",
    "    image_as_array = np.array(image.getdata())\n",
    "    images.append(image_as_array)\n",
    "    return images\n",
    "\n",
    "\n",
    "def BuildDataLoadersFromPngFiles():\n",
    "  for label in range(10):\n",
    "    dir = os.path.join(config.PNG_TRAIN_DATA_DIR, f'{label}')\n",
    "    images = ReadImages(dir)\n",
    "    print(images[0])\n",
    "    input()\n",
    "\n",
    "  pass\n",
    "\n",
    "\n",
    "def BuildDataLoadersFromCsvFiles():\n",
    "  train_images = pd.read_csv(config.TRAIN_DATA_FILE, header=None)\n",
    "  test_images = pd.read_csv(config.TEST_DATA_FILE, header=None)\n",
    "\n",
    "  train_data = np.array(train_images.iloc[:,1:])\n",
    "  train_labels = np.array(train_images.iloc[:,0])\n",
    "  test_data = np.array(test_images.iloc[:,1:])\n",
    "  test_labels = np.array(test_images.iloc[:,0])\n",
    "\n",
    "  train_data = np.reshape(train_data, (len(train_data), 1, 28, 28))\n",
    "  # Normalize the data\n",
    "  train_data = torch.from_numpy(train_data).float()/255\n",
    "  train_labels = torch.from_numpy(np.array(train_labels))\n",
    "  train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "\n",
    "  test_data = np.reshape(test_data, (len(test_data), 1, 28, 28))\n",
    "  # Normalize the data\n",
    "  test_data = torch.from_numpy(test_data).float()/255\n",
    "  test_labels = torch.from_numpy(np.array(test_labels))\n",
    "  test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "  \n",
    "  train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "  test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=True)\n",
    "  return train_loader, test_loader\n",
    "\n",
    "\n",
    "# We first import the data and convert them to a torch version\n",
    "def BuildDataLoaders(use_png_format=USE_PNG_FORMAT):\n",
    "  if use_png_format:\n",
    "    return BuildDataLoadersFromPngFiles()\n",
    "  return BuildDataLoadersFromCsvFiles()\n",
    "\n",
    "\n",
    "train_loader, test_loader = BuildDataLoaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "PR_CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Dropout2d(p=0.2, inplace=False)\n",
      "    (1): Conv2d(1, 8, kernel_size=(3, 3), stride=(3, 3))\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): Dropout2d(p=0.2, inplace=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(16, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten()\n",
      "    (1): Linear(in_features=1536, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adriana/.local/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 49824/60000 (83.04%), Avg. loss: 1.6397, \n",
      "\n",
      "Epoch 2\n",
      "\n",
      "Accuracy: 51054/60000 (85.09%), Avg. loss: 1.6141, \n",
      "\n",
      "Epoch 3\n",
      "\n",
      "Accuracy: 51460/60000 (85.77%), Avg. loss: 1.6056, \n",
      "\n",
      "Epoch 4\n",
      "\n",
      "Accuracy: 51916/60000 (86.53%), Avg. loss: 1.5979, \n",
      "\n",
      "Epoch 5\n",
      "\n",
      "Accuracy: 52120/60000 (86.87%), Avg. loss: 1.5939, \n",
      "\n",
      "Epoch 6\n",
      "\n",
      "Accuracy: 52550/60000 (87.58%), Avg. loss: 1.5872, \n",
      "\n",
      "Epoch 7\n",
      "\n",
      "Accuracy: 52588/60000 (87.65%), Avg. loss: 1.5860, \n",
      "\n",
      "Epoch 8\n",
      "\n",
      "Accuracy: 52794/60000 (87.99%), Avg. loss: 1.5821, \n",
      "\n",
      "Epoch 9\n",
      "\n",
      "Accuracy: 52891/60000 (88.15%), Avg. loss: 1.5804, \n",
      "\n",
      "Epoch 10\n",
      "\n",
      "Accuracy: 52950/60000 (88.25%), Avg. loss: 1.5796, \n",
      "\n",
      "Epoch 11\n",
      "\n",
      "Accuracy: 53024/60000 (88.37%), Avg. loss: 1.5780, \n",
      "\n",
      "Epoch 12\n",
      "\n",
      "Accuracy: 53162/60000 (88.60%), Avg. loss: 1.5758, \n",
      "\n",
      "Epoch 13\n",
      "\n",
      "Accuracy: 53105/60000 (88.51%), Avg. loss: 1.5765, \n",
      "\n",
      "Epoch 14\n",
      "\n",
      "Accuracy: 53269/60000 (88.78%), Avg. loss: 1.5735, \n",
      "\n",
      "Epoch 15\n",
      "\n",
      "Accuracy: 53225/60000 (88.71%), Avg. loss: 1.5739, \n",
      "\n",
      "Epoch 16\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import model_task2c\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Something is not quite right and the values become nan\n",
    "torch.autograd.set_detect_anomaly(True) \n",
    "\n",
    "\n",
    "# Initialize classifier stuff\n",
    "cnn_model = model_task2c.PR_CNN()   # initialization of the model\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "\n",
    "# Compute the accuracy of the model on the given data set\n",
    "def ComputeAccuracy(loader, loss_func='cross_entropy'):\n",
    "    cnn_model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (data, label) in enumerate(loader):\n",
    "            output = cnn_model(data)\n",
    "            \n",
    "            if loss_func == 'cross_entropy':\n",
    "                total_loss += F.cross_entropy(output, label, size_average=False).item()\n",
    "            else:\n",
    "                total_loss += F.nll_loss(output, label, size_average=False).item()\n",
    "\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "\n",
    "    \n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print('Accuracy: {}/{} ({:.2f}%), Avg. loss: {:.4f}, \\n'.format(\n",
    "        correct, \n",
    "        len(loader.dataset), \n",
    "        100. * correct / len(loader.dataset),\n",
    "        average_loss))\n",
    "\n",
    "\n",
    "def train(loss_func='cross_entropy'):\n",
    "    loss_per_epoch = []\n",
    "\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        print(f'Epoch {epoch + 1}')\n",
    "\n",
    "        cnn_model.train()  # activation of the train mode\n",
    "\n",
    "        for _, (data, label) in enumerate(train_loader):\n",
    "            # Set gradients to 0. PyTorch accumulates the gradients \n",
    "            # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = cnn_model(data)\n",
    "\n",
    "            if loss_func == 'cross_entropy':\n",
    "                loss = F.cross_entropy(output, label)\n",
    "            else:\n",
    "                loss = F.nll_loss(output, label)\n",
    "            loss_per_epoch.append(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Compute the accuracy of the model at the end of each epoch\n",
    "        ComputeAccuracy(train_loader, loss_func)\n",
    "\n",
    "    return loss_per_epoch\n",
    "\n",
    "loss_per_epoch = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model on the test dataset\n",
    "ComputeAccuracy(test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
