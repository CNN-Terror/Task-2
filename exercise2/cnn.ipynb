{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_TEST = 1000 # I don't think this actually matters\n",
    "\n",
    "NUMBER_OF_EPOCHS = 25\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0\n",
    "\n",
    "USE_PNG_FORMAT = False\n",
    "\n",
    "# The accuracy is measured on the test data, not the train one\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.01 and BATCH_SIZE=32 I got 76% accuracy after 25 EPOCHS\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.02 and BATCH_SIZE=32 I got 68% accuracy after 9 EPOCHS, 85% accuracy after 25 EPOCHS\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.05 and BATCH_SIZE=32 I got 78% accuracy after 25 EPOCHS\n",
    "# With MOMENTUM=0, LEARNING_RATE=0.1  and BATCH_SIZE=32 I got 70% accuracy after 25 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exercise2_config as config\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(round(datetime.timestamp(datetime.now())))\n",
    "\n",
    "\n",
    "def ReadImages(dir):\n",
    "  images = []\n",
    "  for filename in os.listdir(dir):\n",
    "    image_path = os.path.join(dir, filename)\n",
    "    image = Image.open(image_path)\n",
    "    image_as_array = np.array(image.getdata())\n",
    "    images.append(image_as_array)\n",
    "    return images\n",
    "\n",
    "\n",
    "def BuildDataLoadersFromPngFiles():\n",
    "  for label in range(10):\n",
    "    dir = os.path.join(config.PNG_TRAIN_DATA_DIR, f'{label}')\n",
    "    images = ReadImages(dir)\n",
    "    print(images[0])\n",
    "    input()\n",
    "\n",
    "  pass\n",
    "\n",
    "\n",
    "def BuildDataLoadersFromCsvFiles():\n",
    "  train_images = pd.read_csv(config.TRAIN_DATA_FILE, header=None)\n",
    "  test_images = pd.read_csv(config.TEST_DATA_FILE, header=None)\n",
    "\n",
    "  train_data = np.array(train_images.iloc[:,1:])\n",
    "  train_labels = np.array(train_images.iloc[:,0])\n",
    "  test_data = np.array(test_images.iloc[:,1:])\n",
    "  test_labels = np.array(test_images.iloc[:,0])\n",
    "\n",
    "  train_data = np.reshape(train_data, (len(train_data), 1, 28, 28))\n",
    "  train_data = torch.from_numpy(train_data).float()/255\n",
    "  train_labels = torch.from_numpy(np.array(train_labels))\n",
    "  train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "\n",
    "  test_data = np.reshape(test_data, (len(test_data), 1, 28, 28))\n",
    "  test_data = torch.from_numpy(test_data).float()/255\n",
    "  test_labels = torch.from_numpy(np.array(test_labels))\n",
    "  test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "  \n",
    "  train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "  test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=True)\n",
    "  return train_loader, test_loader\n",
    "\n",
    "\n",
    "# We first import the data and convert them to a torch version\n",
    "def BuildDataLoaders(use_png_format=USE_PNG_FORMAT):\n",
    "  if use_png_format:\n",
    "    return BuildDataLoadersFromPngFiles()\n",
    "  return BuildDataLoadersFromCsvFiles()\n",
    "\n",
    "\n",
    "train_loader, test_loader = BuildDataLoaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Epoch 1\n",
      "\n",
      "Test set: Avg. loss: -0.6361, Accuracy: 6555/10000 (65.55%)\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "Test set: Avg. loss: -0.6755, Accuracy: 6797/10000 (67.97%)\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "Test set: Avg. loss: -0.6782, Accuracy: 6814/10000 (68.14%)\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "Test set: Avg. loss: -0.6835, Accuracy: 6861/10000 (68.61%)\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "Test set: Avg. loss: -0.6821, Accuracy: 6844/10000 (68.44%)\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "Test set: Avg. loss: -0.6862, Accuracy: 6881/10000 (68.81%)\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "Test set: Avg. loss: -0.6896, Accuracy: 6918/10000 (69.18%)\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "Test set: Avg. loss: -0.6910, Accuracy: 6919/10000 (69.19%)\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "Test set: Avg. loss: -0.6911, Accuracy: 6919/10000 (69.19%)\n",
      "\n",
      "Epoch 10\n",
      "\n",
      "Test set: Avg. loss: -0.6948, Accuracy: 6962/10000 (69.62%)\n",
      "\n",
      "Epoch 11\n",
      "\n",
      "Test set: Avg. loss: -0.6957, Accuracy: 6971/10000 (69.71%)\n",
      "\n",
      "Epoch 12\n",
      "\n",
      "Test set: Avg. loss: -0.6963, Accuracy: 6975/10000 (69.75%)\n",
      "\n",
      "Epoch 13\n",
      "\n",
      "Test set: Avg. loss: -0.6970, Accuracy: 6987/10000 (69.87%)\n",
      "\n",
      "Epoch 14\n",
      "\n",
      "Test set: Avg. loss: -0.6967, Accuracy: 6976/10000 (69.76%)\n",
      "\n",
      "Epoch 15\n",
      "\n",
      "Test set: Avg. loss: -0.6953, Accuracy: 6969/10000 (69.69%)\n",
      "\n",
      "Epoch 16\n",
      "\n",
      "Test set: Avg. loss: -0.6986, Accuracy: 6998/10000 (69.98%)\n",
      "\n",
      "Epoch 17\n",
      "\n",
      "Test set: Avg. loss: -0.6981, Accuracy: 6991/10000 (69.91%)\n",
      "\n",
      "Epoch 18\n",
      "\n",
      "Test set: Avg. loss: -0.6994, Accuracy: 7004/10000 (70.04%)\n",
      "\n",
      "Epoch 19\n",
      "\n",
      "Test set: Avg. loss: -0.6986, Accuracy: 6998/10000 (69.98%)\n",
      "\n",
      "Epoch 20\n",
      "\n",
      "Test set: Avg. loss: -0.6986, Accuracy: 6997/10000 (69.97%)\n",
      "\n",
      "Epoch 21\n",
      "\n",
      "Test set: Avg. loss: -0.6999, Accuracy: 7007/10000 (70.07%)\n",
      "\n",
      "Epoch 22\n",
      "\n",
      "Test set: Avg. loss: -0.6987, Accuracy: 6993/10000 (69.93%)\n",
      "\n",
      "Epoch 23\n",
      "\n",
      "Test set: Avg. loss: -0.6981, Accuracy: 6991/10000 (69.91%)\n",
      "\n",
      "Epoch 24\n",
      "\n",
      "Test set: Avg. loss: -0.7009, Accuracy: 7019/10000 (70.19%)\n",
      "\n",
      "Epoch 25\n",
      "\n",
      "Test set: Avg. loss: -0.6993, Accuracy: 7001/10000 (70.01%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import model_task2c\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Something is not quite right and the values become nan\n",
    "torch.autograd.set_detect_anomaly(True) \n",
    "\n",
    "\n",
    "# Initialize classifier stuff\n",
    "cnn_model = model_task2c.PR_CNN()   # initialization of the model\n",
    "cnn_model.train()  # activation of the train mode\n",
    "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "def test():\n",
    "    cnn_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (data, label) in enumerate(test_loader):\n",
    "            output = cnn_model(data)\n",
    "            test_loss += F.nll_loss(output, label, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "def train():\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        print(f'Epoch {epoch + 1}')\n",
    "\n",
    "        for _, (data, label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output = cnn_model(data)\n",
    "            loss = F.nll_loss(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Test the model at the end of each epoch\n",
    "        test()\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
