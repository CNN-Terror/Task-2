{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exercise2_config as config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_images = pd.read_csv(config.TRAIN_DATA_FILE, header=None)\n",
    "test_images = pd.read_csv(config.TEST_DATA_FILE, header=None)\n",
    "\n",
    "train_data = np.array(train_images.iloc[:,1:])\n",
    "train_labels = np.array(train_images.iloc[:,0])\n",
    "test_data = np.array(test_images.iloc[:,1:])\n",
    "test_labels = np.array(test_images.iloc[:,0])\n",
    "\n",
    "\n",
    "# REMOVE THIS\n",
    "#train_data = train_data[:100]\n",
    "#train_labels = train_labels[:100]\n",
    "#test_data = test_data[:100]\n",
    "#test_labels = test_labels[:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state = 1648668123\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Iteration 1, loss = 1.45535309\n",
      "Iteration 1, loss = 0.76012736\n",
      "Iteration 1, loss = 1.45854324\n",
      "Iteration 1, loss = 1.44564497\n",
      "Iteration 1, loss = 1.45110309\n",
      "Iteration 1, loss = 1.45210696\n",
      "Iteration 1, loss = 0.72583594\n",
      "Iteration 1, loss = 0.70579328\n",
      "Iteration 2, loss = 0.81112664\n",
      "Iteration 2, loss = 0.82422204\n",
      "Iteration 2, loss = 0.53430604\n",
      "Iteration 2, loss = 0.81300388\n",
      "Iteration 2, loss = 0.49306965\n",
      "Iteration 2, loss = 0.81517909\n",
      "Iteration 2, loss = 0.49958531\n",
      "Iteration 3, loss = 0.59876658\n",
      "Iteration 2, loss = 0.81276217\n",
      "Iteration 3, loss = 0.49164780\n",
      "Iteration 3, loss = 0.59964268\n",
      "Iteration 3, loss = 0.50235302\n",
      "Iteration 3, loss = 0.61037358\n",
      "Iteration 4, loss = 0.49446797\n",
      "Iteration 3, loss = 0.59739545\n",
      "Iteration 3, loss = 0.60181025\n",
      "Iteration 3, loss = 0.48359069\n",
      "Iteration 4, loss = 0.46798402\n",
      "Iteration 4, loss = 0.49670857\n",
      "Iteration 4, loss = 0.49588940\n",
      "Iteration 4, loss = 0.50479168\n",
      "Iteration 5, loss = 0.43425535\n",
      "Iteration 4, loss = 0.49443799\n",
      "Iteration 4, loss = 0.49758573\n",
      "Iteration 4, loss = 0.46460724\n",
      "Iteration 5, loss = 0.49744983\n",
      "Iteration 5, loss = 0.48398191\n",
      "Iteration 5, loss = 0.43597383\n",
      "Iteration 5, loss = 0.44228305\n",
      "Iteration 6, loss = 0.39459354\n",
      "Iteration 5, loss = 0.43540882\n",
      "Iteration 5, loss = 0.43372915\n",
      "Iteration 5, loss = 0.45401219\n",
      "Iteration 6, loss = 0.47716967\n",
      "Iteration 6, loss = 0.50142272\n",
      "Iteration 6, loss = 0.39267767\n",
      "Iteration 6, loss = 0.39301221\n",
      "Iteration 6, loss = 0.40130288\n",
      "Iteration 7, loss = 0.36500538\n",
      "Iteration 6, loss = 0.43733960\n",
      "Iteration 6, loss = 0.39188560\n",
      "Iteration 7, loss = 0.49714932\n",
      "Iteration 7, loss = 0.49626318\n",
      "Iteration 7, loss = 0.36932730\n",
      "Iteration 7, loss = 0.36273759\n",
      "Iteration 7, loss = 0.36169989\n",
      "Iteration 8, loss = 0.34120302\n",
      "Iteration 8, loss = 0.48786384\n",
      "Iteration 7, loss = 0.36165327\n",
      "Iteration 7, loss = 0.46224626\n",
      "Iteration 8, loss = 0.48948388\n",
      "Iteration 8, loss = 0.33894797\n",
      "Iteration 8, loss = 0.34524091\n",
      "Iteration 8, loss = 0.33784897\n",
      "Iteration 9, loss = 0.32167212\n",
      "Iteration 9, loss = 0.46357147\n",
      "Iteration 8, loss = 0.33892562\n",
      "Iteration 8, loss = 0.45072258\n",
      "Iteration 9, loss = 0.45968226\n",
      "Iteration 9, loss = 0.32133308\n",
      "Iteration 9, loss = 0.32615068\n",
      "Iteration 9, loss = 0.31931765\n",
      "Iteration 10, loss = 0.30814678\n",
      "Iteration 10, loss = 0.46196012\n",
      "Iteration 9, loss = 0.32030022\n",
      "Iteration 10, loss = 0.44380657\n",
      "Iteration 10, loss = 0.30419656\n",
      "Iteration 10, loss = 0.30950187\n",
      "Iteration 9, loss = 0.43551167\n",
      "Iteration 10, loss = 0.30391943\n",
      "Iteration 11, loss = 0.45358641\n",
      "Iteration 11, loss = 0.29406234\n",
      "Iteration 10, loss = 0.30468666\n",
      "Iteration 11, loss = 0.41856682\n",
      "Iteration 11, loss = 0.29624276\n",
      "Iteration 10, loss = 0.44103761\n",
      "Iteration 11, loss = 0.29148568\n",
      "Iteration 11, loss = 0.28907946\n",
      "Iteration 12, loss = 0.47245689\n",
      "Iteration 12, loss = 0.28285438\n",
      "Iteration 11, loss = 0.29244454\n",
      "Iteration 12, loss = 0.41559912\n",
      "Iteration 12, loss = 0.28314074\n",
      "Iteration 11, loss = 0.45328089\n",
      "Iteration 12, loss = 0.27956736\n",
      "Iteration 12, loss = 0.28018396\n",
      "Iteration 13, loss = 0.27343855\n",
      "Iteration 12, loss = 0.28059071\n",
      "Iteration 13, loss = 0.47323905\n",
      "Iteration 13, loss = 0.27352840\n",
      "Iteration 13, loss = 0.45174713\n",
      "Iteration 13, loss = 0.26898161\n",
      "Iteration 12, loss = 0.44165741\n",
      "Iteration 14, loss = 0.26385264\n",
      "Iteration 13, loss = 0.27007495\n",
      "Iteration 13, loss = 0.27052012\n",
      "Iteration 14, loss = 0.43334684\n",
      "Iteration 14, loss = 0.45830782\n",
      "Iteration 14, loss = 0.26094667\n",
      "Iteration 14, loss = 0.26509261\n",
      "Iteration 14, loss = 0.26082562\n",
      "Iteration 13, loss = 0.45035114\n",
      "Iteration 15, loss = 0.25509198\n",
      "Iteration 14, loss = 0.26179702\n",
      "Iteration 15, loss = 0.41562999\n",
      "Iteration 15, loss = 0.45097452\n",
      "Iteration 15, loss = 0.25520732\n",
      "Iteration 15, loss = 0.25367084\n",
      "Iteration 15, loss = 0.25153735\n",
      "Iteration 16, loss = 0.24646593\n",
      "Iteration 14, loss = 0.44362362\n",
      "Iteration 15, loss = 0.25362077\n",
      "Iteration 16, loss = 0.42772456\n",
      "Iteration 16, loss = 0.24660085\n",
      "Iteration 16, loss = 0.43646819\n",
      "Iteration 16, loss = 0.24942839\n",
      "Iteration 17, loss = 0.24140498\n",
      "Iteration 16, loss = 0.24278036\n",
      "Iteration 15, loss = 0.46022560\n",
      "Iteration 16, loss = 0.24475422\n",
      "Iteration 17, loss = 0.23847451\n",
      "Iteration 17, loss = 0.43613177\n",
      "Iteration 17, loss = 0.24173636\n",
      "Iteration 17, loss = 0.43156996\n",
      "Iteration 18, loss = 0.23552710\n",
      "Iteration 18, loss = 0.23328665\n",
      "Iteration 16, loss = 0.47101454\n",
      "Iteration 17, loss = 0.23727727\n",
      "Iteration 17, loss = 0.23753993\n",
      "Iteration 18, loss = 0.42090779\n",
      "Iteration 18, loss = 0.42732496\n",
      "Iteration 19, loss = 0.22647097\n",
      "Iteration 18, loss = 0.23403715\n",
      "Iteration 19, loss = 0.23048754\n",
      "Iteration 18, loss = 0.23069370\n",
      "Iteration 18, loss = 0.23122956\n",
      "Iteration 17, loss = 0.48198974\n",
      "Iteration 19, loss = 0.44798024\n",
      "Iteration 20, loss = 0.22220586\n",
      "Iteration 19, loss = 0.40789985\n",
      "Iteration 20, loss = 0.22374786\n",
      "Iteration 19, loss = 0.22531060\n",
      "Iteration 19, loss = 0.23061233\n",
      "Iteration 19, loss = 0.22738950\n",
      "Iteration 18, loss = 0.45076286\n",
      "Iteration 21, loss = 0.21837240\n",
      "Iteration 20, loss = 0.47617680\n",
      "Iteration 20, loss = 0.21879936\n",
      "Iteration 20, loss = 0.21997645Iteration 20, loss = 0.40412310\n",
      "\n",
      "Iteration 20, loss = 0.22311480\n",
      "Iteration 21, loss = 0.21974810\n",
      "Iteration 19, loss = 0.44958187\n",
      "Iteration 22, loss = 0.21345508\n",
      "Iteration 21, loss = 0.44929154\n",
      "Iteration 21, loss = 0.21527616Iteration 21, loss = 0.21497268\n",
      "\n",
      "Iteration 21, loss = 0.21905708\n",
      "Iteration 22, loss = 0.21439201\n",
      "Iteration 21, loss = 0.41874028\n",
      "Iteration 23, loss = 0.20782976\n",
      "Iteration 20, loss = 0.42942623\n",
      "Iteration 22, loss = 0.21032787\n",
      "Iteration 22, loss = 0.45152311\n",
      "Iteration 22, loss = 0.21106127\n",
      "Iteration 22, loss = 0.21355603\n",
      "Iteration 24, loss = 0.20383881\n",
      "Iteration 23, loss = 0.21073520\n",
      "Iteration 22, loss = 0.41275109\n",
      "Iteration 21, loss = 0.44340283\n",
      "Iteration 23, loss = 0.20696416\n",
      "Iteration 23, loss = 0.42927899\n",
      "Iteration 23, loss = 0.20579186\n",
      "Iteration 25, loss = 0.20040053\n",
      "Iteration 23, loss = 0.20974559\n",
      "Iteration 24, loss = 0.20680240\n",
      "Iteration 23, loss = 0.41270490\n",
      "Iteration 24, loss = 0.20164451\n",
      "Iteration 22, loss = 0.44342582\n",
      "Iteration 24, loss = 0.42251595\n",
      "Iteration 26, loss = 0.19497264\n",
      "Iteration 24, loss = 0.20294899\n",
      "Iteration 24, loss = 0.20528819\n",
      "Iteration 24, loss = 0.40439288\n",
      "Iteration 25, loss = 0.20184774\n",
      "Iteration 25, loss = 0.19817835\n",
      "Iteration 27, loss = 0.19282166\n",
      "Iteration 23, loss = 0.42438896\n",
      "Iteration 25, loss = 0.19900238\n",
      "Iteration 25, loss = 0.41280455\n",
      "Iteration 25, loss = 0.20051964\n",
      "Iteration 25, loss = 0.40821956\n",
      "Iteration 26, loss = 0.19803392\n",
      "Iteration 28, loss = 0.19008733\n",
      "Iteration 26, loss = 0.19330143\n",
      "Iteration 26, loss = 0.19521000\n",
      "Iteration 24, loss = 0.47822063\n",
      "Iteration 26, loss = 0.42657165\n",
      "Iteration 26, loss = 0.19680574\n",
      "Iteration 27, loss = 0.19421878\n",
      "Iteration 26, loss = 0.40368893\n",
      "Iteration 27, loss = 0.19176712\n",
      "Iteration 29, loss = 0.18625480\n",
      "Iteration 27, loss = 0.19158537\n",
      "Iteration 25, loss = 0.43800068\n",
      "Iteration 27, loss = 0.42624232\n",
      "Iteration 27, loss = 0.19452535\n",
      "Iteration 28, loss = 0.18952377\n",
      "Iteration 28, loss = 0.18910573\n",
      "Iteration 27, loss = 0.39368831\n",
      "Iteration 30, loss = 0.18091533\n",
      "Iteration 28, loss = 0.18644160\n",
      "Iteration 26, loss = 0.42494619\n",
      "Iteration 28, loss = 0.19255676\n",
      "Iteration 29, loss = 0.18341050\n",
      "Iteration 28, loss = 0.40794154\n",
      "Iteration 29, loss = 0.18691636\n",
      "Iteration 28, loss = 0.39356037\n",
      "Iteration 31, loss = 0.18008410\n",
      "Iteration 29, loss = 0.18511297\n",
      "Iteration 27, loss = 0.41945485\n",
      "Iteration 30, loss = 0.18047131\n",
      "Iteration 29, loss = 0.18721204\n",
      "Iteration 30, loss = 0.18610703\n",
      "Iteration 29, loss = 0.38050834\n",
      "Iteration 28, loss = 0.39948795\n",
      "Iteration 30, loss = 0.18069689\n",
      "Iteration 29, loss = 0.37571116\n",
      "Iteration 32, loss = 0.17881926\n",
      "Iteration 31, loss = 0.17797991\n",
      "Iteration 31, loss = 0.17993357\n",
      "Iteration 30, loss = 0.18469012\n",
      "Iteration 30, loss = 0.39052423\n",
      "Iteration 29, loss = 0.41759279\n",
      "Iteration 31, loss = 0.17779189\n",
      "Iteration 33, loss = 0.17324632\n",
      "Iteration 32, loss = 0.17463646\n",
      "Iteration 30, loss = 0.38163570\n",
      "Iteration 32, loss = 0.17694415\n",
      "Iteration 31, loss = 0.18170162\n",
      "Iteration 32, loss = 0.17440426\n",
      "Iteration 34, loss = 0.17095462\n",
      "Iteration 30, loss = 0.41561892\n",
      "Iteration 31, loss = 0.38960168\n",
      "Iteration 33, loss = 0.17131224\n",
      "Iteration 33, loss = 0.17511866\n",
      "Iteration 32, loss = 0.17941751\n",
      "Iteration 31, loss = 0.38632221\n",
      "Iteration 33, loss = 0.17186414\n",
      "Iteration 32, loss = 0.37551993\n",
      "Iteration 35, loss = 0.16851074\n",
      "Iteration 34, loss = 0.16898813\n",
      "Iteration 34, loss = 0.17190519\n",
      "Iteration 33, loss = 0.17422552\n",
      "Iteration 31, loss = 0.40551433\n",
      "Iteration 32, loss = 0.39319161\n",
      "Iteration 34, loss = 0.16959313\n",
      "Iteration 35, loss = 0.16985985\n",
      "Iteration 33, loss = 0.39312932\n",
      "Iteration 34, loss = 0.17440951\n",
      "Iteration 35, loss = 0.16702779\n",
      "Iteration 36, loss = 0.16606096\n",
      "Iteration 32, loss = 0.39996532\n",
      "Iteration 33, loss = 0.37644877\n",
      "Iteration 35, loss = 0.16740878\n",
      "Iteration 36, loss = 0.16779071\n",
      "Iteration 34, loss = 0.37128202\n",
      "Iteration 37, loss = 0.16447676\n",
      "Iteration 35, loss = 0.16941469\n",
      "Iteration 36, loss = 0.16442646\n",
      "Iteration 33, loss = 0.40474608\n",
      "Iteration 34, loss = 0.35043744\n",
      "Iteration 36, loss = 0.16407482\n",
      "Iteration 37, loss = 0.16430501\n",
      "Iteration 35, loss = 0.37120043\n",
      "Iteration 38, loss = 0.16069545\n",
      "Iteration 36, loss = 0.16714055\n",
      "Iteration 37, loss = 0.16253213\n",
      "Iteration 34, loss = 0.39538831\n",
      "Iteration 37, loss = 0.16190495\n",
      "Iteration 35, loss = 0.35007299\n",
      "Iteration 38, loss = 0.16107869\n",
      "Iteration 39, loss = 0.16055083\n",
      "Iteration 36, loss = 0.38158664\n",
      "Iteration 37, loss = 0.16656740\n",
      "Iteration 38, loss = 0.16014252\n",
      "Iteration 38, loss = 0.16070132\n",
      "Iteration 39, loss = 0.15972474\n",
      "Iteration 36, loss = 0.40263265\n",
      "Iteration 35, loss = 0.37871473\n",
      "Iteration 40, loss = 0.15735427\n",
      "Iteration 38, loss = 0.16304949\n",
      "Iteration 37, loss = 0.36384249\n",
      "Iteration 40, loss = 0.15760003\n",
      "Iteration 37, loss = 0.38772725\n",
      "Iteration 39, loss = 0.15740437\n",
      "Iteration 39, loss = 0.15601975\n",
      "Iteration 36, loss = 0.39039365\n",
      "Iteration 41, loss = 0.15312296\n",
      "Iteration 39, loss = 0.16138168\n",
      "Iteration 38, loss = 0.36530242\n",
      "Iteration 41, loss = 0.15618164\n",
      "Iteration 38, loss = 0.37878782\n",
      "Iteration 40, loss = 0.15671901\n",
      "Iteration 40, loss = 0.15470003\n",
      "Iteration 37, loss = 0.37077161\n",
      "Iteration 42, loss = 0.15181209\n",
      "Iteration 40, loss = 0.15942798\n",
      "Iteration 39, loss = 0.36817881\n",
      "Iteration 42, loss = 0.15432943\n",
      "Iteration 39, loss = 0.37907931\n",
      "Iteration 41, loss = 0.15204218\n",
      "Iteration 41, loss = 0.15331241\n",
      "Iteration 38, loss = 0.36632404\n",
      "Iteration 41, loss = 0.15710957\n",
      "Iteration 40, loss = 0.37886544\n",
      "Iteration 43, loss = 0.14899720\n",
      "Iteration 40, loss = 0.38268918\n",
      "Iteration 43, loss = 0.15172624\n",
      "Iteration 42, loss = 0.15040103\n",
      "Iteration 42, loss = 0.15499587\n",
      "Iteration 42, loss = 0.15070763\n",
      "Iteration 39, loss = 0.36428466\n",
      "Iteration 41, loss = 0.35631543\n",
      "Iteration 44, loss = 0.14655019\n",
      "Iteration 41, loss = 0.38522969\n",
      "Iteration 44, loss = 0.15123976\n",
      "Iteration 43, loss = 0.14949117\n",
      "Iteration 43, loss = 0.15162333\n",
      "Iteration 40, loss = 0.34325632\n",
      "Iteration 43, loss = 0.14879619\n",
      "Iteration 42, loss = 0.33560243\n",
      "Iteration 42, loss = 0.39018258\n",
      "Iteration 45, loss = 0.14476936\n",
      "Iteration 45, loss = 0.14817717\n",
      "Iteration 44, loss = 0.14955821\n",
      "Iteration 44, loss = 0.14730527\n",
      "Iteration 44, loss = 0.14618480\n",
      "Iteration 41, loss = 0.34893449\n",
      "Iteration 43, loss = 0.35457314\n",
      "Iteration 43, loss = 0.37109525\n",
      "Iteration 46, loss = 0.14339654\n",
      "Iteration 46, loss = 0.14496138\n",
      "Iteration 45, loss = 0.14803400\n",
      "Iteration 45, loss = 0.14415286\n",
      "Iteration 45, loss = 0.14332100\n",
      "Iteration 44, loss = 0.36985195\n",
      "Iteration 44, loss = 0.34240109\n",
      "Iteration 42, loss = 0.34231189\n",
      "Iteration 47, loss = 0.14250232\n",
      "Iteration 47, loss = 0.14404368\n",
      "Iteration 46, loss = 0.14668824\n",
      "Iteration 46, loss = 0.14402957\n",
      "Iteration 46, loss = 0.14115261\n",
      "Iteration 45, loss = 0.34558831\n",
      "Iteration 48, loss = 0.14026111\n",
      "Iteration 45, loss = 0.35999142\n",
      "Iteration 43, loss = 0.36086692\n",
      "Iteration 48, loss = 0.14237003\n",
      "Iteration 47, loss = 0.14127238\n",
      "Iteration 47, loss = 0.14320869\n",
      "Iteration 47, loss = 0.14031348\n",
      "Iteration 46, loss = 0.33014882\n",
      "Iteration 49, loss = 0.14051451\n",
      "Iteration 46, loss = 0.35911497\n",
      "Iteration 44, loss = 0.36837603\n",
      "Iteration 49, loss = 0.13920556\n",
      "Iteration 48, loss = 0.14286992\n",
      "Iteration 48, loss = 0.14136168\n",
      "Iteration 48, loss = 0.13853147\n",
      "Iteration 47, loss = 0.32985698\n",
      "Iteration 50, loss = 0.13915835\n",
      "Iteration 47, loss = 0.34102015\n",
      "Iteration 45, loss = 0.35989071\n",
      "Iteration 50, loss = 0.13988331\n",
      "Iteration 49, loss = 0.13970504\n",
      "Iteration 49, loss = 0.13662538\n",
      "Iteration 49, loss = 0.13803415\n",
      "Iteration 51, loss = 0.13590152\n",
      "Iteration 48, loss = 0.35968872\n",
      "Iteration 48, loss = 0.34779819\n",
      "Iteration 46, loss = 0.35857382\n",
      "Iteration 51, loss = 0.13741133\n",
      "Iteration 50, loss = 0.13489789\n",
      "Iteration 50, loss = 0.13837067\n",
      "Iteration 50, loss = 0.13723618\n",
      "Iteration 52, loss = 0.13381713\n",
      "Iteration 49, loss = 0.34908010\n",
      "Iteration 49, loss = 0.34605387\n",
      "Iteration 47, loss = 0.35846922\n",
      "Iteration 52, loss = 0.13649411\n",
      "Iteration 51, loss = 0.13768689\n",
      "Iteration 51, loss = 0.13480689\n",
      "Iteration 51, loss = 0.13529616\n",
      "Iteration 53, loss = 0.13343661\n",
      "Iteration 50, loss = 0.33186452\n",
      "Iteration 50, loss = 0.33545061\n",
      "Iteration 53, loss = 0.13393396\n",
      "Iteration 48, loss = 0.38156551\n",
      "Iteration 52, loss = 0.13693744\n",
      "Iteration 54, loss = 0.13060368\n",
      "Iteration 52, loss = 0.13366156\n",
      "Iteration 52, loss = 0.13354487\n",
      "Iteration 51, loss = 0.33448661\n",
      "Iteration 51, loss = 0.34007310\n",
      "Iteration 54, loss = 0.13222525\n",
      "Iteration 49, loss = 0.37833000\n",
      "Iteration 53, loss = 0.13514553\n",
      "Iteration 53, loss = 0.13215175\n",
      "Iteration 53, loss = 0.13060628\n",
      "Iteration 55, loss = 0.13013393\n",
      "Iteration 52, loss = 0.32445363\n",
      "Iteration 55, loss = 0.13377998\n",
      "Iteration 54, loss = 0.13311643\n",
      "Iteration 50, loss = 0.39006130\n",
      "Iteration 52, loss = 0.32870350\n",
      "Iteration 54, loss = 0.13019144\n",
      "Iteration 54, loss = 0.12849629\n",
      "Iteration 56, loss = 0.12751938\n",
      "Iteration 53, loss = 0.33006595\n",
      "Iteration 56, loss = 0.12897377\n",
      "Iteration 55, loss = 0.13070821\n",
      "Iteration 53, loss = 0.35788748\n",
      "Iteration 51, loss = 0.42849281\n",
      "Iteration 55, loss = 0.12795867\n",
      "Iteration 57, loss = 0.12728964\n",
      "Iteration 55, loss = 0.12731978\n",
      "Iteration 54, loss = 0.33323663\n",
      "Iteration 57, loss = 0.12762093\n",
      "Iteration 54, loss = 0.37803118\n",
      "Iteration 56, loss = 0.12777871\n",
      "Iteration 52, loss = 0.38507998\n",
      "Iteration 56, loss = 0.12787803\n",
      "Iteration 56, loss = 0.12502533\n",
      "Iteration 58, loss = 0.12468616\n",
      "Iteration 55, loss = 0.36667489\n",
      "Iteration 55, loss = 0.34144014\n",
      "Iteration 58, loss = 0.12746635\n",
      "Iteration 57, loss = 0.12936446\n",
      "Iteration 53, loss = 0.38241692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.12676473\n",
      "Iteration 57, loss = 0.12478446\n",
      "Iteration 59, loss = 0.12355993\n",
      "Iteration 56, loss = 0.35125176\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.025750000000000002, solver=sgd; total time= 2.5min\n",
      "Iteration 59, loss = 0.12500518\n",
      "Iteration 58, loss = 0.12663486\n",
      "Iteration 56, loss = 0.34575053\n",
      "Iteration 58, loss = 0.12327488\n",
      "Iteration 58, loss = 0.12234409\n",
      "Iteration 60, loss = 0.12252432\n",
      "Iteration 60, loss = 0.12364315\n",
      "Iteration 57, loss = 0.35286520\n",
      "Iteration 59, loss = 0.12476374\n",
      "Iteration 1, loss = 0.72937087\n",
      "Iteration 59, loss = 0.12273532\n",
      "Iteration 57, loss = 0.32122154\n",
      "Iteration 59, loss = 0.12245810\n",
      "Iteration 61, loss = 0.12346946\n",
      "Iteration 61, loss = 0.12085149\n",
      "Iteration 60, loss = 0.12330028\n",
      "Iteration 58, loss = 0.34706001\n",
      "Iteration 2, loss = 0.50388737\n",
      "Iteration 60, loss = 0.12167594\n",
      "Iteration 58, loss = 0.32807403\n",
      "Iteration 60, loss = 0.11901090\n",
      "Iteration 62, loss = 0.12167291\n",
      "Iteration 61, loss = 0.12339475\n",
      "Iteration 62, loss = 0.11907215\n",
      "Iteration 59, loss = 0.34804291\n",
      "Iteration 3, loss = 0.52552144\n",
      "Iteration 61, loss = 0.12175929\n",
      "Iteration 61, loss = 0.11899702\n",
      "Iteration 62, loss = 0.12183174\n",
      "Iteration 63, loss = 0.11867404\n",
      "Iteration 63, loss = 0.11925553\n",
      "Iteration 59, loss = 0.32742042\n",
      "Iteration 60, loss = 0.34334726\n",
      "Iteration 4, loss = 0.51126659\n",
      "Iteration 63, loss = 0.12032164\n",
      "Iteration 62, loss = 0.12019474\n",
      "Iteration 64, loss = 0.11609494\n",
      "Iteration 62, loss = 0.11680416\n",
      "Iteration 64, loss = 0.11776913\n",
      "Iteration 60, loss = 0.31750401\n",
      "Iteration 61, loss = 0.34061853\n",
      "Iteration 5, loss = 0.46324510\n",
      "Iteration 64, loss = 0.11715147\n",
      "Iteration 65, loss = 0.11471931\n",
      "Iteration 63, loss = 0.11702955\n",
      "Iteration 65, loss = 0.11652886\n",
      "Iteration 63, loss = 0.11689129\n",
      "Iteration 6, loss = 0.46750712\n",
      "Iteration 61, loss = 0.32491837\n",
      "Iteration 62, loss = 0.33137818\n",
      "Iteration 65, loss = 0.11724373\n",
      "Iteration 64, loss = 0.11710472\n",
      "Iteration 66, loss = 0.11602294\n",
      "Iteration 66, loss = 0.11330670\n",
      "Iteration 7, loss = 0.43335420\n",
      "Iteration 64, loss = 0.11415817\n",
      "Iteration 62, loss = 0.33418537\n",
      "Iteration 63, loss = 0.36165992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 66, loss = 0.11521167\n",
      "Iteration 67, loss = 0.11378967\n",
      "Iteration 65, loss = 0.11540534\n",
      "Iteration 67, loss = 0.11295818\n",
      "Iteration 8, loss = 0.42300262\n",
      "Iteration 65, loss = 0.11290744\n",
      "Iteration 63, loss = 0.34483027\n",
      "Iteration 67, loss = 0.11590534\n",
      "Iteration 68, loss = 0.11330137\n",
      "Iteration 66, loss = 0.11456433\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.025750000000000002, solver=sgd; total time= 2.9min\n",
      "Iteration 68, loss = 0.11129999\n",
      "Iteration 66, loss = 0.11229856\n",
      "Iteration 9, loss = 0.44249630\n",
      "Iteration 64, loss = 0.33939056\n",
      "Iteration 68, loss = 0.11384520\n",
      "Iteration 69, loss = 0.11263436\n",
      "Iteration 67, loss = 0.11229407\n",
      "Iteration 67, loss = 0.11267767\n",
      "Iteration 69, loss = 0.10930293\n",
      "Iteration 10, loss = 0.42495033\n",
      "Iteration 65, loss = 0.34825430\n",
      "Iteration 70, loss = 0.11080842\n",
      "Iteration 69, loss = 0.11310075\n",
      "Iteration 1, loss = 0.73376149\n",
      "Iteration 68, loss = 0.11060378\n",
      "Iteration 70, loss = 0.10788998\n",
      "Iteration 68, loss = 0.11076829\n",
      "Iteration 2, loss = 0.51634104\n",
      "Iteration 66, loss = 0.35014973\n",
      "Iteration 69, loss = 0.11010817\n",
      "Iteration 11, loss = 0.44493996\n",
      "Iteration 71, loss = 0.11157594\n",
      "Iteration 70, loss = 0.11071252\n",
      "Iteration 71, loss = 0.10681446\n",
      "Iteration 69, loss = 0.10977337\n",
      "Iteration 3, loss = 0.48807430\n",
      "Iteration 70, loss = 0.10823559\n",
      "Iteration 67, loss = 0.32720169\n",
      "Iteration 72, loss = 0.10996437\n",
      "Iteration 71, loss = 0.11077600\n",
      "Iteration 12, loss = 0.43797908\n",
      "Iteration 72, loss = 0.10585218\n",
      "Iteration 70, loss = 0.10698715\n",
      "Iteration 4, loss = 0.49230498\n",
      "Iteration 71, loss = 0.10749938\n",
      "Iteration 68, loss = 0.33631914\n",
      "Iteration 73, loss = 0.10858890Iteration 73, loss = 0.10614389\n",
      "\n",
      "Iteration 72, loss = 0.10907236\n",
      "Iteration 13, loss = 0.43201107\n",
      "Iteration 5, loss = 0.49614561\n",
      "Iteration 71, loss = 0.10584333\n",
      "Iteration 72, loss = 0.10702034\n",
      "Iteration 69, loss = 0.35187018\n",
      "Iteration 74, loss = 0.10325373\n",
      "Iteration 74, loss = 0.10581723\n",
      "Iteration 73, loss = 0.10793304\n",
      "Iteration 6, loss = 0.47487524\n",
      "Iteration 14, loss = 0.42045099\n",
      "Iteration 72, loss = 0.10718053\n",
      "Iteration 73, loss = 0.10549853\n",
      "Iteration 70, loss = 0.33716225\n",
      "Iteration 75, loss = 0.10253984\n",
      "Iteration 7, loss = 0.47851880\n",
      "Iteration 75, loss = 0.10623737\n",
      "Iteration 74, loss = 0.10655111\n",
      "Iteration 73, loss = 0.10418915\n",
      "Iteration 15, loss = 0.43835169\n",
      "Iteration 74, loss = 0.10496329\n",
      "Iteration 71, loss = 0.33917546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 76, loss = 0.10176776\n",
      "Iteration 8, loss = 0.46142264\n",
      "Iteration 74, loss = 0.10293429\n",
      "Iteration 76, loss = 0.10496559\n",
      "Iteration 75, loss = 0.10569164\n",
      "Iteration 16, loss = 0.44307174\n",
      "Iteration 75, loss = 0.10295460\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.025750000000000002, solver=sgd; total time= 3.3min\n",
      "Iteration 77, loss = 0.10199514\n",
      "Iteration 75, loss = 0.10162212\n",
      "Iteration 77, loss = 0.10498460\n",
      "Iteration 76, loss = 0.10480701\n",
      "Iteration 9, loss = 0.48965537\n",
      "Iteration 17, loss = 0.43156141\n",
      "Iteration 76, loss = 0.10231200\n",
      "Iteration 78, loss = 0.10095195\n",
      "Iteration 76, loss = 0.10135204\n",
      "Iteration 77, loss = 0.10367628\n",
      "Iteration 78, loss = 0.10268094\n",
      "Iteration 1, loss = 0.84938475\n",
      "Iteration 10, loss = 0.49690383\n",
      "Iteration 77, loss = 0.10203625\n",
      "Iteration 18, loss = 0.41132201\n",
      "Iteration 77, loss = 0.10092694\n",
      "Iteration 79, loss = 0.10099601\n",
      "Iteration 78, loss = 0.10285143\n",
      "Iteration 79, loss = 0.10212824\n",
      "Iteration 2, loss = 0.72950063\n",
      "Iteration 11, loss = 0.45902771\n",
      "Iteration 78, loss = 0.10099877\n",
      "Iteration 78, loss = 0.10004999\n",
      "Iteration 80, loss = 0.09923176\n",
      "Iteration 19, loss = 0.39161778\n",
      "Iteration 79, loss = 0.10130839\n",
      "Iteration 80, loss = 0.10000187\n",
      "Iteration 79, loss = 0.10053205\n",
      "Iteration 3, loss = 0.71786205\n",
      "Iteration 79, loss = 0.09852558\n",
      "Iteration 12, loss = 0.43353356\n",
      "Iteration 81, loss = 0.09857206\n",
      "Iteration 20, loss = 0.40219096\n",
      "Iteration 81, loss = 0.09923981\n",
      "Iteration 80, loss = 0.10188121\n",
      "Iteration 80, loss = 0.09762286\n",
      "Iteration 80, loss = 0.09927202\n",
      "Iteration 4, loss = 0.66902458\n",
      "Iteration 82, loss = 0.09711767\n",
      "Iteration 13, loss = 0.42815940\n",
      "Iteration 21, loss = 0.38524930\n",
      "Iteration 82, loss = 0.09854074\n",
      "Iteration 81, loss = 0.09814457\n",
      "Iteration 81, loss = 0.09902930\n",
      "Iteration 83, loss = 0.09629295\n",
      "Iteration 81, loss = 0.09835642\n",
      "Iteration 14, loss = 0.40389128\n",
      "Iteration 5, loss = 0.63934178\n",
      "Iteration 83, loss = 0.09660931\n",
      "Iteration 22, loss = 0.39095807\n",
      "Iteration 82, loss = 0.09740544\n",
      "Iteration 84, loss = 0.09627002\n",
      "Iteration 82, loss = 0.09848395\n",
      "Iteration 82, loss = 0.09615232\n",
      "Iteration 15, loss = 0.44523985\n",
      "Iteration 6, loss = 0.68081121\n",
      "Iteration 84, loss = 0.09617236\n",
      "Iteration 23, loss = 0.39392134\n",
      "Iteration 83, loss = 0.09534321\n",
      "Iteration 83, loss = 0.09765276\n",
      "Iteration 85, loss = 0.09465312\n",
      "Iteration 85, loss = 0.09622402\n",
      "Iteration 83, loss = 0.09602959\n",
      "Iteration 16, loss = 0.40317371\n",
      "Iteration 7, loss = 0.64812922\n",
      "Iteration 24, loss = 0.39893187\n",
      "Iteration 84, loss = 0.09431675\n",
      "Iteration 84, loss = 0.09614862\n",
      "Iteration 86, loss = 0.09173457\n",
      "Iteration 84, loss = 0.09558153\n",
      "Iteration 86, loss = 0.09315989\n",
      "Iteration 8, loss = 0.67027896\n",
      "Iteration 17, loss = 0.42963704\n",
      "Iteration 85, loss = 0.09273619\n",
      "Iteration 25, loss = 0.39990990\n",
      "Iteration 85, loss = 0.09575972\n",
      "Iteration 87, loss = 0.09267811\n",
      "Iteration 85, loss = 0.09402136\n",
      "Iteration 87, loss = 0.09274107\n",
      "Iteration 9, loss = 0.64703467\n",
      "Iteration 18, loss = 0.41665163\n",
      "Iteration 86, loss = 0.09342892\n",
      "Iteration 26, loss = 0.40220103\n",
      "Iteration 86, loss = 0.09177315\n",
      "Iteration 88, loss = 0.09110104\n",
      "Iteration 88, loss = 0.09308592\n",
      "Iteration 86, loss = 0.09296899\n",
      "Iteration 10, loss = 0.61766653\n",
      "Iteration 87, loss = 0.09275588\n",
      "Iteration 19, loss = 0.39492139\n",
      "Iteration 87, loss = 0.09072688\n",
      "Iteration 27, loss = 0.39062901\n",
      "Iteration 89, loss = 0.09229871\n",
      "Iteration 87, loss = 0.09191761\n",
      "Iteration 89, loss = 0.09016828\n",
      "Iteration 11, loss = 0.59542238\n",
      "Iteration 88, loss = 0.09293133\n",
      "Iteration 20, loss = 0.40636462\n",
      "Iteration 88, loss = 0.08984695\n",
      "Iteration 28, loss = 0.39402561\n",
      "Iteration 88, loss = 0.09099232\n",
      "Iteration 90, loss = 0.09162853\n",
      "Iteration 89, loss = 0.09263394\n",
      "Iteration 12, loss = 0.63354928\n",
      "Iteration 90, loss = 0.09047945\n",
      "Iteration 21, loss = 0.40466378\n",
      "Iteration 89, loss = 0.08940174\n",
      "Iteration 89, loss = 0.09095525\n",
      "Iteration 29, loss = 0.40251636\n",
      "Iteration 90, loss = 0.09132466\n",
      "Iteration 91, loss = 0.09053322\n",
      "Iteration 91, loss = 0.09067328\n",
      "Iteration 13, loss = 0.61679651\n",
      "Iteration 22, loss = 0.41484068\n",
      "Iteration 90, loss = 0.08992763\n",
      "Iteration 90, loss = 0.08916911\n",
      "Iteration 30, loss = 0.37719469\n",
      "Iteration 91, loss = 0.08904579\n",
      "Iteration 92, loss = 0.08776643\n",
      "Iteration 23, loss = 0.38553405\n",
      "Iteration 92, loss = 0.09020340\n",
      "Iteration 14, loss = 0.58736196\n",
      "Iteration 91, loss = 0.08854777\n",
      "Iteration 91, loss = 0.08711121\n",
      "Iteration 31, loss = 0.37371714\n",
      "Iteration 92, loss = 0.08956945\n",
      "Iteration 93, loss = 0.08669828\n",
      "Iteration 24, loss = 0.36773613\n",
      "Iteration 92, loss = 0.08602829\n",
      "Iteration 15, loss = 0.65834976\n",
      "Iteration 93, loss = 0.08973603\n",
      "Iteration 92, loss = 0.08764903\n",
      "Iteration 32, loss = 0.35724792\n",
      "Iteration 94, loss = 0.08560724\n",
      "Iteration 93, loss = 0.08913592\n",
      "Iteration 93, loss = 0.08648144\n",
      "Iteration 25, loss = 0.37619624\n",
      "Iteration 16, loss = 0.62077081\n",
      "Iteration 94, loss = 0.08744330\n",
      "Iteration 93, loss = 0.08765867\n",
      "Iteration 33, loss = 0.34570988\n",
      "Iteration 94, loss = 0.08653116\n",
      "Iteration 95, loss = 0.08479643\n",
      "Iteration 26, loss = 0.41163129\n",
      "Iteration 17, loss = 0.64346828\n",
      "Iteration 94, loss = 0.08789195\n",
      "Iteration 95, loss = 0.08834984\n",
      "Iteration 94, loss = 0.08603624\n",
      "Iteration 95, loss = 0.08578888\n",
      "Iteration 34, loss = 0.34662802\n",
      "Iteration 27, loss = 0.38949528\n",
      "Iteration 96, loss = 0.08517963\n",
      "Iteration 95, loss = 0.08772708\n",
      "Iteration 18, loss = 0.59186165\n",
      "Iteration 96, loss = 0.08680818\n",
      "Iteration 96, loss = 0.08502766\n",
      "Iteration 95, loss = 0.08657101\n",
      "Iteration 35, loss = 0.37214623\n",
      "Iteration 19, loss = 0.64615314\n",
      "Iteration 97, loss = 0.08419907\n",
      "Iteration 96, loss = 0.08764882\n",
      "Iteration 28, loss = 0.38696079\n",
      "Iteration 97, loss = 0.08500012\n",
      "Iteration 97, loss = 0.08327735\n",
      "Iteration 96, loss = 0.08372720\n",
      "Iteration 20, loss = 0.57987417\n",
      "Iteration 97, loss = 0.08588199\n",
      "Iteration 29, loss = 0.40213918\n",
      "Iteration 36, loss = 0.36320842\n",
      "Iteration 98, loss = 0.08302724\n",
      "Iteration 98, loss = 0.08223535\n",
      "Iteration 98, loss = 0.08374957\n",
      "Iteration 97, loss = 0.08391176\n",
      "Iteration 21, loss = 0.60040633\n",
      "Iteration 98, loss = 0.08562239\n",
      "Iteration 30, loss = 0.38206018\n",
      "Iteration 99, loss = 0.08222210\n",
      "Iteration 99, loss = 0.08184037\n",
      "Iteration 37, loss = 0.35605352\n",
      "Iteration 99, loss = 0.08361185\n",
      "Iteration 98, loss = 0.08244166\n",
      "Iteration 22, loss = 0.62394878\n",
      "Iteration 99, loss = 0.08413892\n",
      "Iteration 100, loss = 0.08062578\n",
      "Iteration 38, loss = 0.34504662\n",
      "Iteration 31, loss = 0.34804135\n",
      "Iteration 100, loss = 0.08114760\n",
      "Iteration 99, loss = 0.08253045\n",
      "Iteration 100, loss = 0.08373514\n",
      "Iteration 23, loss = 0.62481860\n",
      "Iteration 100, loss = 0.08361543\n",
      "Iteration 101, loss = 0.08096289\n",
      "Iteration 32, loss = 0.35652431\n",
      "Iteration 101, loss = 0.08078435\n",
      "Iteration 24, loss = 0.60127823\n",
      "Iteration 39, loss = 0.36491878\n",
      "Iteration 100, loss = 0.08135331\n",
      "Iteration 101, loss = 0.08380781\n",
      "Iteration 101, loss = 0.08251288\n",
      "Iteration 102, loss = 0.07967765\n",
      "Iteration 102, loss = 0.08052532\n",
      "Iteration 33, loss = 0.37125198\n",
      "Iteration 25, loss = 0.58462117\n",
      "Iteration 102, loss = 0.08051942\n",
      "Iteration 102, loss = 0.08149839\n",
      "Iteration 101, loss = 0.08025106\n",
      "Iteration 40, loss = 0.34175527\n",
      "Iteration 103, loss = 0.07931137\n",
      "Iteration 26, loss = 0.58405249\n",
      "Iteration 103, loss = 0.08103381\n",
      "Iteration 103, loss = 0.08164438\n",
      "Iteration 103, loss = 0.08034748\n",
      "Iteration 34, loss = 0.37785019\n",
      "Iteration 104, loss = 0.07793783\n",
      "Iteration 102, loss = 0.07921645\n",
      "Iteration 41, loss = 0.33367584\n",
      "Iteration 27, loss = 0.53472176\n",
      "Iteration 104, loss = 0.08034681\n",
      "Iteration 104, loss = 0.07949621\n",
      "Iteration 35, loss = 0.37265712\n",
      "Iteration 105, loss = 0.07719831\n",
      "Iteration 104, loss = 0.08144365\n",
      "Iteration 42, loss = 0.33414277\n",
      "Iteration 103, loss = 0.07864792\n",
      "Iteration 28, loss = 0.53304621\n",
      "Iteration 105, loss = 0.07952652\n",
      "Iteration 106, loss = 0.07694087\n",
      "Iteration 36, loss = 0.37649128\n",
      "Iteration 105, loss = 0.07980790\n",
      "Iteration 105, loss = 0.07859484\n",
      "Iteration 104, loss = 0.07847574\n",
      "Iteration 43, loss = 0.36036111\n",
      "Iteration 29, loss = 0.56078986\n",
      "Iteration 107, loss = 0.07584645\n",
      "Iteration 106, loss = 0.07845885\n",
      "Iteration 106, loss = 0.07819323\n",
      "Iteration 106, loss = 0.07834516\n",
      "Iteration 37, loss = 0.35891172\n",
      "Iteration 105, loss = 0.07732289\n",
      "Iteration 30, loss = 0.56237255\n",
      "Iteration 44, loss = 0.33765205\n",
      "Iteration 108, loss = 0.07498590\n",
      "Iteration 107, loss = 0.07755302\n",
      "Iteration 107, loss = 0.07717769\n",
      "Iteration 107, loss = 0.07801268\n",
      "Iteration 38, loss = 0.37729757\n",
      "Iteration 109, loss = 0.07463897\n",
      "Iteration 31, loss = 0.52519280\n",
      "Iteration 106, loss = 0.07793399\n",
      "Iteration 108, loss = 0.07734412\n",
      "Iteration 45, loss = 0.33232926\n",
      "Iteration 108, loss = 0.07598126\n",
      "Iteration 108, loss = 0.07790378\n",
      "Iteration 110, loss = 0.07428353\n",
      "Iteration 39, loss = 0.36263386\n",
      "Iteration 109, loss = 0.07648746\n",
      "Iteration 107, loss = 0.07619443\n",
      "Iteration 32, loss = 0.50337454\n",
      "Iteration 46, loss = 0.34295507\n",
      "Iteration 109, loss = 0.07507363\n",
      "Iteration 111, loss = 0.07382397\n",
      "Iteration 109, loss = 0.07785371\n",
      "Iteration 40, loss = 0.35574509\n",
      "Iteration 33, loss = 0.49911012\n",
      "Iteration 110, loss = 0.07550292\n",
      "Iteration 47, loss = 0.32322247\n",
      "Iteration 108, loss = 0.07527569\n",
      "Iteration 110, loss = 0.07472164\n",
      "Iteration 112, loss = 0.07435971\n",
      "Iteration 110, loss = 0.07688816\n",
      "Iteration 41, loss = 0.34109828\n",
      "Iteration 111, loss = 0.07474676\n",
      "Iteration 34, loss = 0.52573029\n",
      "Iteration 111, loss = 0.07491576\n",
      "Iteration 48, loss = 0.32826951\n",
      "Iteration 113, loss = 0.07286086\n",
      "Iteration 111, loss = 0.07578862\n",
      "Iteration 109, loss = 0.07441294\n",
      "Iteration 112, loss = 0.07433388\n",
      "Iteration 42, loss = 0.34755428\n",
      "Iteration 114, loss = 0.07126403\n",
      "Iteration 35, loss = 0.54289310\n",
      "Iteration 112, loss = 0.07381909\n",
      "Iteration 112, loss = 0.07488632\n",
      "Iteration 49, loss = 0.34248075\n",
      "Iteration 110, loss = 0.07396904\n",
      "Iteration 113, loss = 0.07427385\n",
      "Iteration 43, loss = 0.34768757\n",
      "Iteration 115, loss = 0.07114975\n",
      "Iteration 113, loss = 0.07323751\n",
      "Iteration 36, loss = 0.49615362\n",
      "Iteration 113, loss = 0.07475712\n",
      "Iteration 50, loss = 0.33435269\n",
      "Iteration 114, loss = 0.07348459\n",
      "Iteration 111, loss = 0.07313717\n",
      "Iteration 44, loss = 0.36063598\n",
      "Iteration 116, loss = 0.06997203\n",
      "Iteration 114, loss = 0.07311325\n",
      "Iteration 37, loss = 0.51557953\n",
      "Iteration 114, loss = 0.07234889\n",
      "Iteration 115, loss = 0.07267357\n",
      "Iteration 51, loss = 0.34328652\n",
      "Iteration 112, loss = 0.07262475\n",
      "Iteration 116, loss = 0.07191940\n",
      "Iteration 45, loss = 0.35208418\n",
      "Iteration 115, loss = 0.07336716\n",
      "Iteration 117, loss = 0.06957425\n",
      "Iteration 115, loss = 0.07185276\n",
      "Iteration 38, loss = 0.52825334\n",
      "Iteration 117, loss = 0.07096030\n",
      "Iteration 52, loss = 0.33638213\n",
      "Iteration 113, loss = 0.07174737\n",
      "Iteration 46, loss = 0.36872379\n",
      "Iteration 116, loss = 0.07262218\n",
      "Iteration 118, loss = 0.07043965\n",
      "Iteration 118, loss = 0.06902043\n",
      "Iteration 116, loss = 0.07039875\n",
      "Iteration 39, loss = 0.53370457\n",
      "Iteration 53, loss = 0.35068550\n",
      "Iteration 119, loss = 0.07024162\n",
      "Iteration 47, loss = 0.35550576\n",
      "Iteration 114, loss = 0.07184357\n",
      "Iteration 119, loss = 0.06837123\n",
      "Iteration 117, loss = 0.07208641\n",
      "Iteration 117, loss = 0.07107598\n",
      "Iteration 40, loss = 0.57725955\n",
      "Iteration 54, loss = 0.34785285\n",
      "Iteration 120, loss = 0.06930962\n",
      "Iteration 115, loss = 0.07116911\n",
      "Iteration 120, loss = 0.06778404\n",
      "Iteration 48, loss = 0.35059258\n",
      "Iteration 118, loss = 0.06970966\n",
      "Iteration 118, loss = 0.07114880\n",
      "Iteration 41, loss = 0.53908439\n",
      "Iteration 121, loss = 0.06930070\n",
      "Iteration 55, loss = 0.34803834\n",
      "Iteration 121, loss = 0.06748802\n",
      "Iteration 119, loss = 0.06838309\n",
      "Iteration 116, loss = 0.07046614\n",
      "Iteration 49, loss = 0.35162353\n",
      "Iteration 119, loss = 0.07057806\n",
      "Iteration 42, loss = 0.53281371\n",
      "Iteration 122, loss = 0.06847230\n",
      "Iteration 122, loss = 0.06773636\n",
      "Iteration 56, loss = 0.33767796\n",
      "Iteration 120, loss = 0.06828556\n",
      "Iteration 117, loss = 0.06931357\n",
      "Iteration 43, loss = 0.56501169\n",
      "Iteration 50, loss = 0.34729217\n",
      "Iteration 120, loss = 0.07035928\n",
      "Iteration 123, loss = 0.06772095\n",
      "Iteration 123, loss = 0.06699725\n",
      "Iteration 121, loss = 0.06845051\n",
      "Iteration 118, loss = 0.06906844\n",
      "Iteration 44, loss = 0.56064727\n",
      "Iteration 57, loss = 0.34136431\n",
      "Iteration 51, loss = 0.33486096\n",
      "Iteration 121, loss = 0.06932066\n",
      "Iteration 124, loss = 0.06693420\n",
      "Iteration 124, loss = 0.06595914\n",
      "Iteration 119, loss = 0.06875993\n",
      "Iteration 45, loss = 0.56389946\n",
      "Iteration 122, loss = 0.06690742\n",
      "Iteration 58, loss = 0.32601362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 125, loss = 0.06630151\n",
      "Iteration 52, loss = 0.33174621\n",
      "Iteration 122, loss = 0.06911374\n",
      "Iteration 125, loss = 0.06604728\n",
      "Iteration 46, loss = 0.51715346\n",
      "Iteration 120, loss = 0.06856760\n",
      "Iteration 123, loss = 0.06665350\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.025750000000000002, solver=sgd; total time= 2.8min\n",
      "Iteration 126, loss = 0.06689068\n",
      "Iteration 53, loss = 0.33715062\n",
      "Iteration 123, loss = 0.06844622\n",
      "Iteration 121, loss = 0.06729006\n",
      "Iteration 124, loss = 0.06632040\n",
      "Iteration 127, loss = 0.06608427\n",
      "Iteration 47, loss = 0.50892283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 126, loss = 0.06500740\n",
      "Iteration 124, loss = 0.06774534\n",
      "Iteration 54, loss = 0.31682125\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.0505, solver=sgd; total time= 2.1min\n",
      "Iteration 128, loss = 0.06602614\n",
      "Iteration 122, loss = 0.06642662\n",
      "Iteration 125, loss = 0.06533447\n",
      "Iteration 1, loss = 0.81874242\n",
      "Iteration 127, loss = 0.06550604\n",
      "Iteration 125, loss = 0.06888664\n",
      "Iteration 55, loss = 0.32321753\n",
      "Iteration 129, loss = 0.06484011\n",
      "Iteration 123, loss = 0.06611839\n",
      "Iteration 126, loss = 0.06508933\n",
      "Iteration 128, loss = 0.06420158\n",
      "Iteration 1, loss = 0.79750928\n",
      "Iteration 2, loss = 0.66072357\n",
      "Iteration 126, loss = 0.06717332\n",
      "Iteration 56, loss = 0.31323479\n",
      "Iteration 130, loss = 0.06487566\n",
      "Iteration 124, loss = 0.06629091\n",
      "Iteration 129, loss = 0.06370659\n",
      "Iteration 127, loss = 0.06488478\n",
      "Iteration 2, loss = 0.68291961\n",
      "Iteration 3, loss = 0.64786099\n",
      "Iteration 127, loss = 0.06638311\n",
      "Iteration 57, loss = 0.32085328\n",
      "Iteration 131, loss = 0.06480535\n",
      "Iteration 125, loss = 0.06539709\n",
      "Iteration 130, loss = 0.06306784\n",
      "Iteration 3, loss = 0.67263690\n",
      "Iteration 128, loss = 0.06503163\n",
      "Iteration 4, loss = 0.65904619\n",
      "Iteration 128, loss = 0.06641949\n",
      "Iteration 132, loss = 0.06386073\n",
      "Iteration 58, loss = 0.31666926\n",
      "Iteration 126, loss = 0.06539988\n",
      "Iteration 131, loss = 0.06247163\n",
      "Iteration 4, loss = 0.67050579\n",
      "Iteration 5, loss = 0.73381982\n",
      "Iteration 129, loss = 0.06448506\n",
      "Iteration 129, loss = 0.06568793\n",
      "Iteration 133, loss = 0.06394297\n",
      "Iteration 59, loss = 0.30966766\n",
      "Iteration 127, loss = 0.06449448\n",
      "Iteration 132, loss = 0.06279931\n",
      "Iteration 6, loss = 0.71649829\n",
      "Iteration 5, loss = 0.61760645\n",
      "Iteration 130, loss = 0.06362116\n",
      "Iteration 130, loss = 0.06458774\n",
      "Iteration 134, loss = 0.06312651\n",
      "Iteration 60, loss = 0.31352291\n",
      "Iteration 133, loss = 0.06272927\n",
      "Iteration 128, loss = 0.06396742\n",
      "Iteration 7, loss = 0.67002768\n",
      "Iteration 6, loss = 0.59241096\n",
      "Iteration 131, loss = 0.06312393\n",
      "Iteration 131, loss = 0.06389691\n",
      "Iteration 61, loss = 0.31112353\n",
      "Iteration 135, loss = 0.06257158\n",
      "Iteration 129, loss = 0.06358592\n",
      "Iteration 134, loss = 0.06145728\n",
      "Iteration 8, loss = 0.66110165\n",
      "Iteration 132, loss = 0.06261950\n",
      "Iteration 7, loss = 0.66041999\n",
      "Iteration 132, loss = 0.06404399\n",
      "Iteration 136, loss = 0.06174061\n",
      "Iteration 62, loss = 0.31417128\n",
      "Iteration 9, loss = 0.69384567\n",
      "Iteration 135, loss = 0.06112958\n",
      "Iteration 130, loss = 0.06375145\n",
      "Iteration 8, loss = 0.66208723\n",
      "Iteration 133, loss = 0.06189537\n",
      "Iteration 133, loss = 0.06269365\n",
      "Iteration 137, loss = 0.06119805\n",
      "Iteration 10, loss = 0.79172764\n",
      "Iteration 63, loss = 0.30834883\n",
      "Iteration 136, loss = 0.06057493\n",
      "Iteration 131, loss = 0.06323266\n",
      "Iteration 134, loss = 0.06154152\n",
      "Iteration 9, loss = 0.61031035\n",
      "Iteration 138, loss = 0.06106107\n",
      "Iteration 134, loss = 0.06239822\n",
      "Iteration 11, loss = 0.69904035\n",
      "Iteration 64, loss = 0.31291657\n",
      "Iteration 132, loss = 0.06227355\n",
      "Iteration 137, loss = 0.06051903\n",
      "Iteration 135, loss = 0.06158963\n",
      "Iteration 139, loss = 0.06074124\n",
      "Iteration 135, loss = 0.06210373\n",
      "Iteration 12, loss = 0.72864319\n",
      "Iteration 10, loss = 0.57454052\n",
      "Iteration 65, loss = 0.32453951\n",
      "Iteration 133, loss = 0.06282002\n",
      "Iteration 136, loss = 0.06056532\n",
      "Iteration 138, loss = 0.06040888\n",
      "Iteration 136, loss = 0.06197446\n",
      "Iteration 140, loss = 0.06067804\n",
      "Iteration 13, loss = 0.66833290\n",
      "Iteration 11, loss = 0.60821334\n",
      "Iteration 66, loss = 0.32794344\n",
      "Iteration 134, loss = 0.06174949\n",
      "Iteration 137, loss = 0.06024451\n",
      "Iteration 139, loss = 0.05994950\n",
      "Iteration 141, loss = 0.06039050\n",
      "Iteration 137, loss = 0.06135077\n",
      "Iteration 14, loss = 0.60755725\n",
      "Iteration 12, loss = 0.61299744\n",
      "Iteration 67, loss = 0.32741074\n",
      "Iteration 135, loss = 0.06095912\n",
      "Iteration 138, loss = 0.05997805\n",
      "Iteration 140, loss = 0.05980396\n",
      "Iteration 142, loss = 0.05972503\n",
      "Iteration 138, loss = 0.06096519\n",
      "Iteration 15, loss = 0.61054057\n",
      "Iteration 13, loss = 0.56844194\n",
      "Iteration 68, loss = 0.34133441\n",
      "Iteration 136, loss = 0.06040855\n",
      "Iteration 139, loss = 0.05920093\n",
      "Iteration 141, loss = 0.05904443\n",
      "Iteration 143, loss = 0.06007008\n",
      "Iteration 139, loss = 0.06040981\n",
      "Iteration 16, loss = 0.63308392\n",
      "Iteration 14, loss = 0.56717405\n",
      "Iteration 137, loss = 0.06033710\n",
      "Iteration 140, loss = 0.05900641\n",
      "Iteration 69, loss = 0.34065233\n",
      "Iteration 142, loss = 0.05882445\n",
      "Iteration 144, loss = 0.05894048\n",
      "Iteration 17, loss = 0.58920048\n",
      "Iteration 140, loss = 0.05976794\n",
      "Iteration 15, loss = 0.55046673\n",
      "Iteration 138, loss = 0.05997757\n",
      "Iteration 70, loss = 0.32824299\n",
      "Iteration 141, loss = 0.05848050\n",
      "Iteration 143, loss = 0.05810764\n",
      "Iteration 145, loss = 0.05877661\n",
      "Iteration 18, loss = 0.59482058\n",
      "Iteration 141, loss = 0.05998387\n",
      "Iteration 16, loss = 0.55366829\n",
      "Iteration 146, loss = 0.05849145\n",
      "Iteration 139, loss = 0.06065083\n",
      "Iteration 144, loss = 0.05792663\n",
      "Iteration 142, loss = 0.05953678\n",
      "Iteration 71, loss = 0.35537044\n",
      "Iteration 19, loss = 0.59035158\n",
      "Iteration 142, loss = 0.05979825\n",
      "Iteration 143, loss = 0.05887454\n",
      "Iteration 147, loss = 0.05779813\n",
      "Iteration 145, loss = 0.05764438\n",
      "Iteration 17, loss = 0.55462241\n",
      "Iteration 140, loss = 0.05940076\n",
      "Iteration 72, loss = 0.33526581\n",
      "Iteration 20, loss = 0.58629988\n",
      "Iteration 144, loss = 0.05808225\n",
      "Iteration 143, loss = 0.05932312\n",
      "Iteration 148, loss = 0.05858800\n",
      "Iteration 146, loss = 0.05722777\n",
      "Iteration 141, loss = 0.05929083\n",
      "Iteration 18, loss = 0.54037010\n",
      "Iteration 73, loss = 0.32110143\n",
      "Iteration 21, loss = 0.61727901\n",
      "Iteration 145, loss = 0.05743878\n",
      "Iteration 144, loss = 0.05855853\n",
      "Iteration 149, loss = 0.05808374\n",
      "Iteration 147, loss = 0.05704213\n",
      "Iteration 142, loss = 0.05911646\n",
      "Iteration 74, loss = 0.33075314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.55534505\n",
      "Iteration 22, loss = 0.56562574\n",
      "Iteration 146, loss = 0.05762901\n",
      "Iteration 150, loss = 0.05753413Iteration 145, loss = 0.05859596\n",
      "\n",
      "Iteration 148, loss = 0.05678116\n",
      "Iteration 143, loss = 0.05857200\n",
      "Iteration 20, loss = 0.56732567\n",
      "Iteration 23, loss = 0.55179873\n",
      "Iteration 147, loss = 0.05730569\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.025750000000000002, solver=sgd; total time= 3.4min\n",
      "Iteration 149, loss = 0.05640224\n",
      "Iteration 151, loss = 0.05695796\n",
      "Iteration 144, loss = 0.05811111\n",
      "Iteration 146, loss = 0.05831327\n",
      "Iteration 21, loss = 0.54193322\n",
      "Iteration 24, loss = 0.51489294\n",
      "Iteration 148, loss = 0.05690205\n",
      "Iteration 145, loss = 0.05773150\n",
      "Iteration 152, loss = 0.05663831\n",
      "Iteration 150, loss = 0.05642877\n",
      "Iteration 147, loss = 0.05778040\n",
      "Iteration 146, loss = 0.05735496\n",
      "Iteration 22, loss = 0.53562493\n",
      "Iteration 149, loss = 0.05618820\n",
      "Iteration 1, loss = 0.84239061\n",
      "Iteration 153, loss = 0.05671098\n",
      "Iteration 25, loss = 0.54352204\n",
      "Iteration 148, loss = 0.05714223\n",
      "Iteration 151, loss = 0.05680431\n",
      "Iteration 147, loss = 0.05741954\n",
      "Iteration 23, loss = 0.50990749\n",
      "Iteration 150, loss = 0.05599642\n",
      "Iteration 154, loss = 0.05597489\n",
      "Iteration 152, loss = 0.05602059\n",
      "Iteration 149, loss = 0.05730517\n",
      "Iteration 2, loss = 0.73094615\n",
      "Iteration 26, loss = 0.59634785\n",
      "Iteration 148, loss = 0.05733849\n",
      "Iteration 24, loss = 0.57963120\n",
      "Iteration 151, loss = 0.05600832\n",
      "Iteration 155, loss = 0.05598144\n",
      "Iteration 153, loss = 0.05544308\n",
      "Iteration 150, loss = 0.05682900\n",
      "Iteration 3, loss = 0.68553393\n",
      "Iteration 27, loss = 0.56151900\n",
      "Iteration 149, loss = 0.05709985\n",
      "Iteration 25, loss = 0.55548281\n",
      "Iteration 154, loss = 0.05525726\n",
      "Iteration 151, loss = 0.05692994\n",
      "Iteration 156, loss = 0.05600671\n",
      "Iteration 152, loss = 0.05572172\n",
      "Iteration 150, loss = 0.05662959\n",
      "Iteration 4, loss = 0.63280468\n",
      "Iteration 28, loss = 0.55513819\n",
      "Iteration 155, loss = 0.05507215\n",
      "Iteration 26, loss = 0.53876151\n",
      "Iteration 151, loss = 0.05685210\n",
      "Iteration 152, loss = 0.05730257\n",
      "Iteration 157, loss = 0.05522536\n",
      "Iteration 153, loss = 0.05532157\n",
      "Iteration 5, loss = 0.67231084\n",
      "Iteration 29, loss = 0.54072511\n",
      "Iteration 156, loss = 0.05472166\n",
      "Iteration 152, loss = 0.05607478\n",
      "Iteration 153, loss = 0.05655831\n",
      "Iteration 158, loss = 0.05467910\n",
      "Iteration 27, loss = 0.52211674\n",
      "Iteration 154, loss = 0.05532014\n",
      "Iteration 153, loss = 0.05569128\n",
      "Iteration 6, loss = 0.66979764\n",
      "Iteration 157, loss = 0.05448367\n",
      "Iteration 30, loss = 0.60490399\n",
      "Iteration 154, loss = 0.05629387\n",
      "Iteration 28, loss = 0.55447149\n",
      "Iteration 159, loss = 0.05481332\n",
      "Iteration 155, loss = 0.05495725\n",
      "Iteration 158, loss = 0.05444869\n",
      "Iteration 154, loss = 0.05544843\n",
      "Iteration 7, loss = 0.67970436\n",
      "Iteration 31, loss = 0.59493596\n",
      "Iteration 155, loss = 0.05575379\n",
      "Iteration 29, loss = 0.53706893\n",
      "Iteration 159, loss = 0.05461186\n",
      "Iteration 160, loss = 0.05426628\n",
      "Iteration 156, loss = 0.05511814\n",
      "Iteration 155, loss = 0.05534851\n",
      "Iteration 8, loss = 0.62813655\n",
      "Iteration 156, loss = 0.05568233\n",
      "Iteration 32, loss = 0.60090605\n",
      "Iteration 160, loss = 0.05400594\n",
      "Iteration 161, loss = 0.05484582\n",
      "Iteration 30, loss = 0.56063132\n",
      "Iteration 156, loss = 0.05510237\n",
      "Iteration 9, loss = 0.60062030\n",
      "Iteration 157, loss = 0.05447089\n",
      "Iteration 157, loss = 0.05509389\n",
      "Iteration 161, loss = 0.05390344\n",
      "Iteration 33, loss = 0.59602155\n",
      "Iteration 162, loss = 0.05415774\n",
      "Iteration 157, loss = 0.05492415\n",
      "Iteration 31, loss = 0.55414779\n",
      "Iteration 10, loss = 0.60694337\n",
      "Iteration 158, loss = 0.05414616\n",
      "Iteration 162, loss = 0.05333344\n",
      "Iteration 158, loss = 0.05526627\n",
      "Iteration 34, loss = 0.60083348\n",
      "Iteration 163, loss = 0.05414707\n",
      "Iteration 158, loss = 0.05489597\n",
      "Iteration 11, loss = 0.66201600\n",
      "Iteration 32, loss = 0.54454089\n",
      "Iteration 159, loss = 0.05409249\n",
      "Iteration 163, loss = 0.05317481\n",
      "Iteration 159, loss = 0.05495971\n",
      "Iteration 35, loss = 0.55623225\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 164, loss = 0.05377900\n",
      "Iteration 159, loss = 0.05453940\n",
      "Iteration 164, loss = 0.05296802\n",
      "Iteration 12, loss = 0.63156596\n",
      "Iteration 160, loss = 0.05407771\n",
      "Iteration 33, loss = 0.48636506\n",
      "Iteration 160, loss = 0.05430826\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.0505, solver=sgd; total time= 1.6min\n",
      "Iteration 165, loss = 0.05348770\n",
      "Iteration 165, loss = 0.05296666\n",
      "Iteration 13, loss = 0.58987212\n",
      "Iteration 160, loss = 0.05436380\n",
      "Iteration 161, loss = 0.05417472\n",
      "Iteration 34, loss = 0.52113292\n",
      "Iteration 161, loss = 0.05406921\n",
      "Iteration 166, loss = 0.05252350\n",
      "Iteration 166, loss = 0.05352818\n",
      "Iteration 14, loss = 0.60568609\n",
      "Iteration 162, loss = 0.05393707\n",
      "Iteration 161, loss = 0.05460464\n",
      "Iteration 35, loss = 0.55459465\n",
      "Iteration 162, loss = 0.05487899\n",
      "Iteration 1, loss = 0.84395580\n",
      "Iteration 167, loss = 0.05275278\n",
      "Iteration 167, loss = 0.05306763\n",
      "Iteration 15, loss = 0.55641047\n",
      "Iteration 162, loss = 0.05430144\n",
      "Iteration 163, loss = 0.05355017\n",
      "Iteration 36, loss = 0.54301884\n",
      "Iteration 163, loss = 0.05380219\n",
      "Iteration 2, loss = 0.76738601\n",
      "Iteration 168, loss = 0.05267193\n",
      "Iteration 168, loss = 0.05288918\n",
      "Iteration 163, loss = 0.05399712\n",
      "Iteration 16, loss = 0.53369944\n",
      "Iteration 164, loss = 0.05333918\n",
      "Iteration 169, loss = 0.05262995\n",
      "Iteration 164, loss = 0.05374199\n",
      "Iteration 37, loss = 0.50072975\n",
      "Iteration 3, loss = 0.70816062\n",
      "Iteration 169, loss = 0.05268297\n",
      "Iteration 164, loss = 0.05345409\n",
      "Iteration 170, loss = 0.05209855\n",
      "Iteration 17, loss = 0.53209898\n",
      "Iteration 165, loss = 0.05342281\n",
      "Iteration 165, loss = 0.05306282\n",
      "Iteration 38, loss = 0.52541581\n",
      "Iteration 4, loss = 0.72100335\n",
      "Iteration 170, loss = 0.05247471\n",
      "Iteration 165, loss = 0.05340612\n",
      "Iteration 171, loss = 0.05177747\n",
      "Iteration 18, loss = 0.52725750\n",
      "Iteration 166, loss = 0.05300865\n",
      "Iteration 5, loss = 0.76237119\n",
      "Iteration 166, loss = 0.05318672\n",
      "Iteration 39, loss = 0.52974377\n",
      "Iteration 172, loss = 0.05155471\n",
      "Iteration 171, loss = 0.05225757\n",
      "Iteration 166, loss = 0.05319418\n",
      "Iteration 167, loss = 0.05302552\n",
      "Iteration 19, loss = 0.53047864\n",
      "Iteration 6, loss = 0.77908604\n",
      "Iteration 167, loss = 0.05304616\n",
      "Iteration 173, loss = 0.05146593\n",
      "Iteration 40, loss = 0.53815695\n",
      "Iteration 172, loss = 0.05205500\n",
      "Iteration 167, loss = 0.05329789\n",
      "Iteration 168, loss = 0.05277510\n",
      "Iteration 7, loss = 0.74148379\n",
      "Iteration 168, loss = 0.05294767\n",
      "Iteration 174, loss = 0.05127251\n",
      "Iteration 20, loss = 0.49482590\n",
      "Iteration 173, loss = 0.05190147\n",
      "Iteration 41, loss = 0.52131266\n",
      "Iteration 168, loss = 0.05296987\n",
      "Iteration 175, loss = 0.05128199\n",
      "Iteration 169, loss = 0.05235985\n",
      "Iteration 8, loss = 0.71755267\n",
      "Iteration 169, loss = 0.05323931\n",
      "Iteration 21, loss = 0.53775337\n",
      "Iteration 174, loss = 0.05174382\n",
      "Iteration 42, loss = 0.49392256\n",
      "Iteration 169, loss = 0.05258844\n",
      "Iteration 176, loss = 0.05121974\n",
      "Iteration 170, loss = 0.05257371\n",
      "Iteration 170, loss = 0.05293343\n",
      "Iteration 9, loss = 0.68895774\n",
      "Iteration 22, loss = 0.51233128\n",
      "Iteration 175, loss = 0.05163708\n",
      "Iteration 177, loss = 0.05136530\n",
      "Iteration 43, loss = 0.50388848\n",
      "Iteration 170, loss = 0.05223949\n",
      "Iteration 171, loss = 0.05213337\n",
      "Iteration 10, loss = 0.71899651\n",
      "Iteration 171, loss = 0.05233073\n",
      "Iteration 176, loss = 0.05158414\n",
      "Iteration 23, loss = 0.52634062\n",
      "Iteration 178, loss = 0.05085138\n",
      "Iteration 44, loss = 0.51508397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 171, loss = 0.05227060\n",
      "Iteration 172, loss = 0.05202644\n",
      "Iteration 172, loss = 0.05204121\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.0505, solver=sgd; total time= 2.1min\n",
      "Iteration 179, loss = 0.05067246\n",
      "Iteration 11, loss = 0.74644895\n",
      "Iteration 177, loss = 0.05157589\n",
      "Iteration 24, loss = 0.56243247\n",
      "Iteration 172, loss = 0.05201700\n",
      "Iteration 173, loss = 0.05163325\n",
      "Iteration 173, loss = 0.05183965\n",
      "Iteration 12, loss = 0.69694495\n",
      "Iteration 180, loss = 0.05075463\n",
      "Iteration 25, loss = 0.51077985\n",
      "Iteration 178, loss = 0.05168137\n",
      "Iteration 173, loss = 0.05179680\n",
      "Iteration 1, loss = 1.07009041\n",
      "Iteration 174, loss = 0.05191401\n",
      "Iteration 174, loss = 0.05154561\n",
      "Iteration 181, loss = 0.05076195\n",
      "Iteration 13, loss = 0.73127592\n",
      "Iteration 26, loss = 0.51770985\n",
      "Iteration 174, loss = 0.05191204\n",
      "Iteration 179, loss = 0.05133105\n",
      "Iteration 2, loss = 0.92887992\n",
      "Iteration 175, loss = 0.05156223\n",
      "Iteration 175, loss = 0.05158198\n",
      "Iteration 182, loss = 0.05054568\n",
      "Iteration 14, loss = 0.74320730\n",
      "Iteration 27, loss = 0.53922784\n",
      "Iteration 3, loss = 0.87521984\n",
      "Iteration 175, loss = 0.05162892\n",
      "Iteration 180, loss = 0.05104372\n",
      "Iteration 176, loss = 0.05158666\n",
      "Iteration 176, loss = 0.05127266\n",
      "Iteration 15, loss = 0.71024868\n",
      "Iteration 183, loss = 0.05021789\n",
      "Iteration 28, loss = 0.54356970\n",
      "Iteration 176, loss = 0.05159497\n",
      "Iteration 4, loss = 0.91266758\n",
      "Iteration 177, loss = 0.05168944\n",
      "Iteration 181, loss = 0.05109523\n",
      "Iteration 177, loss = 0.05110007\n",
      "Iteration 16, loss = 0.62458167\n",
      "Iteration 184, loss = 0.05010951\n",
      "Iteration 178, loss = 0.05120666\n",
      "Iteration 29, loss = 0.56031035\n",
      "Iteration 177, loss = 0.05133195\n",
      "Iteration 5, loss = 0.87774838\n",
      "Iteration 182, loss = 0.05096857\n",
      "Iteration 178, loss = 0.05094211\n",
      "Iteration 17, loss = 0.63364123\n",
      "Iteration 179, loss = 0.05098269\n",
      "Iteration 185, loss = 0.05020276\n",
      "Iteration 30, loss = 0.53350334\n",
      "Iteration 183, loss = 0.05071429\n",
      "Iteration 179, loss = 0.05083755\n",
      "Iteration 6, loss = 0.86613374\n",
      "Iteration 178, loss = 0.05107286\n",
      "Iteration 18, loss = 0.67606985\n",
      "Iteration 180, loss = 0.05089001\n",
      "Iteration 31, loss = 0.56161200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 186, loss = 0.05002861\n",
      "Iteration 180, loss = 0.05079654\n",
      "Iteration 184, loss = 0.05043887\n",
      "Iteration 181, loss = 0.05102865\n",
      "Iteration 7, loss = 0.75609770\n",
      "Iteration 179, loss = 0.05094140\n",
      "Iteration 19, loss = 0.60540783\n",
      "Iteration 187, loss = 0.04964705\n",
      "Iteration 185, loss = 0.05054317\n",
      "Iteration 181, loss = 0.05052932\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.0505, solver=sgd; total time= 1.5min\n",
      "Iteration 182, loss = 0.05070596\n",
      "Iteration 8, loss = 0.78374640\n",
      "Iteration 180, loss = 0.05079010\n",
      "Iteration 20, loss = 0.61466044\n",
      "Iteration 188, loss = 0.04960409\n",
      "Iteration 186, loss = 0.05078141\n",
      "Iteration 182, loss = 0.05047737\n",
      "Iteration 183, loss = 0.05046897\n",
      "Iteration 9, loss = 0.81283576\n",
      "Iteration 181, loss = 0.05072964\n",
      "Iteration 189, loss = 0.04946518\n",
      "Iteration 187, loss = 0.05024574\n",
      "Iteration 21, loss = 0.65760407\n",
      "Iteration 183, loss = 0.05032889\n",
      "Iteration 1, loss = 0.92398268\n",
      "Iteration 184, loss = 0.05028740\n",
      "Iteration 182, loss = 0.05073394\n",
      "Iteration 10, loss = 0.79873249\n",
      "Iteration 188, loss = 0.05009644\n",
      "Iteration 190, loss = 0.04931683\n",
      "Iteration 22, loss = 0.65223457\n",
      "Iteration 184, loss = 0.05023001\n",
      "Iteration 2, loss = 0.84202787\n",
      "Iteration 185, loss = 0.05019711\n",
      "Iteration 183, loss = 0.05045148\n",
      "Iteration 189, loss = 0.04983290\n",
      "Iteration 11, loss = 0.75777094\n",
      "Iteration 191, loss = 0.04929963\n",
      "Iteration 23, loss = 0.61376112\n",
      "Iteration 185, loss = 0.05002164\n",
      "Iteration 190, loss = 0.04973745\n",
      "Iteration 3, loss = 0.80378332\n",
      "Iteration 186, loss = 0.05022684\n",
      "Iteration 184, loss = 0.05069972\n",
      "Iteration 12, loss = 0.73404042\n",
      "Iteration 192, loss = 0.04928305\n",
      "Iteration 186, loss = 0.04990604\n",
      "Iteration 24, loss = 0.61587743\n",
      "Iteration 191, loss = 0.04964914\n",
      "Iteration 187, loss = 0.05006392\n",
      "Iteration 4, loss = 0.85503139\n",
      "Iteration 185, loss = 0.05034578\n",
      "Iteration 13, loss = 0.79613301\n",
      "Iteration 192, loss = 0.04963967\n",
      "Iteration 193, loss = 0.04907531\n",
      "Iteration 187, loss = 0.04984131\n",
      "Iteration 25, loss = 0.57750209\n",
      "Iteration 188, loss = 0.04985818\n",
      "Iteration 5, loss = 0.80642359\n",
      "Iteration 186, loss = 0.05011105\n",
      "Iteration 193, loss = 0.04962129\n",
      "Iteration 14, loss = 0.78468728\n",
      "Iteration 194, loss = 0.04887455\n",
      "Iteration 188, loss = 0.04985050\n",
      "Iteration 26, loss = 0.64149769\n",
      "Iteration 189, loss = 0.05004255\n",
      "Iteration 187, loss = 0.05002413\n",
      "Iteration 6, loss = 0.76967739\n",
      "Iteration 194, loss = 0.04947159\n",
      "Iteration 15, loss = 0.73057677\n",
      "Iteration 189, loss = 0.04967560\n",
      "Iteration 195, loss = 0.04876004\n",
      "Iteration 27, loss = 0.59817648\n",
      "Iteration 195, loss = 0.04927523\n",
      "Iteration 190, loss = 0.04994933\n",
      "Iteration 188, loss = 0.04985786\n",
      "Iteration 7, loss = 0.80566941\n",
      "Iteration 16, loss = 0.72555436\n",
      "Iteration 190, loss = 0.05008767\n",
      "Iteration 196, loss = 0.04873501\n",
      "Iteration 196, loss = 0.04927147\n",
      "Iteration 28, loss = 0.57532015\n",
      "Iteration 191, loss = 0.04959480\n",
      "Iteration 189, loss = 0.04977256\n",
      "Iteration 8, loss = 0.73339927\n",
      "Iteration 17, loss = 0.71033790\n",
      "Iteration 191, loss = 0.04943242\n",
      "Iteration 197, loss = 0.04919175\n",
      "Iteration 197, loss = 0.04860220\n",
      "Iteration 29, loss = 0.56773932\n",
      "Iteration 190, loss = 0.04964175\n",
      "Iteration 192, loss = 0.04949521\n",
      "Iteration 9, loss = 0.76890101\n",
      "Iteration 18, loss = 0.72434111\n",
      "Iteration 198, loss = 0.04901788\n",
      "Iteration 192, loss = 0.04930864\n",
      "Iteration 198, loss = 0.04851379\n",
      "Iteration 30, loss = 0.57746471\n",
      "Iteration 191, loss = 0.04956061\n",
      "Iteration 193, loss = 0.04928792\n",
      "Iteration 19, loss = 0.69355487\n",
      "Iteration 10, loss = 0.71378467\n",
      "Iteration 199, loss = 0.04908747\n",
      "Iteration 193, loss = 0.04925375\n",
      "Iteration 31, loss = 0.57280358\n",
      "Iteration 199, loss = 0.04834556\n",
      "Iteration 192, loss = 0.04939679\n",
      "Iteration 20, loss = 0.73514319\n",
      "Iteration 194, loss = 0.04914094\n",
      "Iteration 194, loss = 0.04912934\n",
      "Iteration 11, loss = 0.71020036\n",
      "Iteration 200, loss = 0.04885661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adriana/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.53143261\n",
      "Iteration 200, loss = 0.04824773\n",
      "Iteration 193, loss = 0.04936107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adriana/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.71229534\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.001, solver=sgd; total time= 8.6min\n",
      "Iteration 195, loss = 0.04901438\n",
      "Iteration 195, loss = 0.04905724\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.001, solver=sgd; total time= 8.6min\n",
      "Iteration 12, loss = 0.75583028\n",
      "Iteration 33, loss = 0.52991554\n",
      "Iteration 194, loss = 0.04924889\n",
      "Iteration 22, loss = 0.75292041\n",
      "Iteration 196, loss = 0.04890079\n",
      "Iteration 196, loss = 0.04892698\n",
      "Iteration 13, loss = 0.73131556\n",
      "Iteration 1, loss = 0.87691847\n",
      "Iteration 195, loss = 0.04907046\n",
      "Iteration 34, loss = 0.52930328\n",
      "Iteration 197, loss = 0.04879048\n",
      "Iteration 23, loss = 0.81194860\n",
      "Iteration 14, loss = 0.75991274\n",
      "Iteration 197, loss = 0.04893654\n",
      "Iteration 2, loss = 0.79237367\n",
      "Iteration 1, loss = 0.92631340\n",
      "Iteration 35, loss = 0.51749618\n",
      "Iteration 196, loss = 0.04891444\n",
      "Iteration 198, loss = 0.04866774\n",
      "Iteration 15, loss = 0.70253689\n",
      "Iteration 24, loss = 0.79319502\n",
      "Iteration 198, loss = 0.04879900\n",
      "Iteration 199, loss = 0.04857669\n",
      "Iteration 2, loss = 0.86915504\n",
      "Iteration 3, loss = 0.80239275\n",
      "Iteration 36, loss = 0.51220775\n",
      "Iteration 197, loss = 0.04888746\n",
      "Iteration 16, loss = 0.78081280\n",
      "Iteration 25, loss = 0.76820798\n",
      "Iteration 200, loss = 0.04848714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adriana/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 199, loss = 0.04867566\n",
      "Iteration 3, loss = 0.80746471\n",
      "Iteration 4, loss = 0.80366698\n",
      "Iteration 37, loss = 0.54006876\n",
      "Iteration 17, loss = 0.75089614\n",
      "Iteration 198, loss = 0.04900086\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.001, solver=sgd; total time= 8.8min\n",
      "Iteration 26, loss = 0.79109228\n",
      "Iteration 200, loss = 0.04859314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adriana/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.74080643\n",
      "Iteration 4, loss = 0.80444512\n",
      "Iteration 38, loss = 0.60303208\n",
      "Iteration 199, loss = 0.04891479\n",
      "Iteration 5, loss = 0.75149628\n",
      "Iteration 27, loss = 0.75525453\n",
      "Iteration 19, loss = 0.70374874\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.001, solver=sgd; total time= 8.9min\n",
      "Iteration 200, loss = 0.04880343\n",
      "Iteration 5, loss = 0.82594123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adriana/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.97713278\n",
      "Iteration 39, loss = 0.56764868Iteration 6, loss = 0.82226951\n",
      "\n",
      "Iteration 28, loss = 0.81646602\n",
      "Iteration 20, loss = 0.65997820\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.001, solver=sgd; total time= 8.9min\n",
      "Iteration 6, loss = 0.76388738\n",
      "Iteration 2, loss = 0.92887886\n",
      "Iteration 7, loss = 0.70803460\n",
      "Iteration 40, loss = 0.56119816\n",
      "Iteration 29, loss = 0.75386623\n",
      "Iteration 21, loss = 0.64028858\n",
      "Iteration 1, loss = 1.03598608\n",
      "Iteration 8, loss = 0.81058356\n",
      "Iteration 7, loss = 0.86176551\n",
      "Iteration 3, loss = 0.81662835\n",
      "Iteration 30, loss = 0.79542359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.52243337\n",
      "Iteration 22, loss = 0.65287424\n",
      "Iteration 1, loss = 0.95315669\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.07525000000000001, solver=sgd; total time= 1.5min\n",
      "Iteration 9, loss = 0.75683353\n",
      "Iteration 8, loss = 0.81050623\n",
      "Iteration 2, loss = 0.97011424\n",
      "Iteration 4, loss = 0.80410958\n",
      "Iteration 42, loss = 0.54855496\n",
      "Iteration 23, loss = 0.62865006\n",
      "Iteration 10, loss = 0.70596142\n",
      "Iteration 2, loss = 0.90676112\n",
      "Iteration 5, loss = 0.77565392\n",
      "Iteration 1, loss = 1.05699036\n",
      "Iteration 9, loss = 0.78557416\n",
      "Iteration 3, loss = 0.89696936\n",
      "Iteration 24, loss = 0.65283033\n",
      "Iteration 43, loss = 0.58422983\n",
      "Iteration 11, loss = 0.72231645\n",
      "Iteration 3, loss = 0.91812527\n",
      "Iteration 2, loss = 1.17759397\n",
      "Iteration 6, loss = 0.84492964\n",
      "Iteration 10, loss = 0.76113056\n",
      "Iteration 25, loss = 0.63181834\n",
      "Iteration 4, loss = 0.94245284\n",
      "Iteration 44, loss = 0.55631826\n",
      "Iteration 3, loss = 0.96662564\n",
      "Iteration 12, loss = 0.76426184\n",
      "Iteration 4, loss = 0.90719478\n",
      "Iteration 11, loss = 0.71088022\n",
      "Iteration 26, loss = 0.71749647\n",
      "Iteration 7, loss = 0.90854211\n",
      "Iteration 45, loss = 0.55193284\n",
      "Iteration 4, loss = 0.99639268\n",
      "Iteration 5, loss = 1.05303710\n",
      "Iteration 27, loss = 0.68387647\n",
      "Iteration 12, loss = 0.68485528\n",
      "Iteration 5, loss = 0.90280055\n",
      "Iteration 13, loss = 0.68524243Iteration 8, loss = 0.87467409\n",
      "\n",
      "Iteration 5, loss = 1.02149746\n",
      "Iteration 46, loss = 0.55243944\n",
      "Iteration 6, loss = 0.99877030\n",
      "Iteration 28, loss = 0.67761905\n",
      "Iteration 13, loss = 0.70730826\n",
      "Iteration 14, loss = 0.65113355\n",
      "Iteration 9, loss = 0.88923629\n",
      "Iteration 6, loss = 0.90494935\n",
      "Iteration 6, loss = 0.92310510\n",
      "Iteration 47, loss = 0.50021866\n",
      "Iteration 29, loss = 0.67832289\n",
      "Iteration 7, loss = 0.96159211\n",
      "Iteration 14, loss = 0.68099508\n",
      "Iteration 7, loss = 0.86936714\n",
      "Iteration 10, loss = 0.82844402\n",
      "Iteration 15, loss = 0.67556683\n",
      "Iteration 7, loss = 0.92635967\n",
      "Iteration 48, loss = 0.52246335\n",
      "Iteration 15, loss = 0.66716707\n",
      "Iteration 30, loss = 0.68380650\n",
      "Iteration 8, loss = 0.99210799\n",
      "Iteration 8, loss = 0.87462755\n",
      "Iteration 8, loss = 0.91662326\n",
      "Iteration 11, loss = 0.80665625\n",
      "Iteration 16, loss = 0.73468838\n",
      "Iteration 49, loss = 0.50986111\n",
      "Iteration 31, loss = 0.67544345\n",
      "Iteration 16, loss = 0.65193986\n",
      "Iteration 9, loss = 0.86772379\n",
      "Iteration 9, loss = 0.95009216\n",
      "Iteration 50, loss = 0.52818658\n",
      "Iteration 9, loss = 0.82976100\n",
      "Iteration 12, loss = 0.88710742\n",
      "Iteration 17, loss = 0.76180797\n",
      "Iteration 10, loss = 0.92256551\n",
      "Iteration 32, loss = 0.69359880\n",
      "Iteration 17, loss = 0.64904849\n",
      "Iteration 51, loss = 0.52821135\n",
      "Iteration 10, loss = 0.99790320\n",
      "Iteration 10, loss = 0.98311426\n",
      "Iteration 13, loss = 0.91897243\n",
      "Iteration 18, loss = 0.71945278\n",
      "Iteration 33, loss = 0.64195408\n",
      "Iteration 11, loss = 0.90376974\n",
      "Iteration 52, loss = 0.49343541\n",
      "Iteration 18, loss = 0.64904248\n",
      "Iteration 14, loss = 0.94941342\n",
      "Iteration 11, loss = 0.98168787\n",
      "Iteration 11, loss = 1.02779684\n",
      "Iteration 19, loss = 0.67749533\n",
      "Iteration 53, loss = 0.46526907\n",
      "Iteration 12, loss = 0.89535636\n",
      "Iteration 34, loss = 0.68540003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.61099862\n",
      "Iteration 15, loss = 1.01184204\n",
      "Iteration 12, loss = 1.08190599\n",
      "Iteration 20, loss = 0.60213033\n",
      "Iteration 12, loss = 1.01899858\n",
      "Iteration 54, loss = 0.45152540\n",
      "Iteration 13, loss = 0.88474806\n",
      "Iteration 20, loss = 0.65728264\n",
      "Iteration 16, loss = 1.03136331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.07525000000000001, solver=sgd; total time= 1.6min\n",
      "Iteration 13, loss = 1.04103190\n",
      "Iteration 21, loss = 0.66852994\n",
      "Iteration 55, loss = 0.47116815Iteration 13, loss = 0.98352828\n",
      "\n",
      "Iteration 14, loss = 0.96400502\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.07525000000000001, solver=sgd; total time=  47.9s\n",
      "Iteration 21, loss = 0.64604804\n",
      "Iteration 22, loss = 0.67706758\n",
      "Iteration 14, loss = 0.98114762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.49561077\n",
      "Iteration 14, loss = 0.99101534\n",
      "Iteration 15, loss = 0.81243358\n",
      "Iteration 1, loss = 1.10016220\n",
      "Iteration 22, loss = 0.65509992\n",
      "Iteration 23, loss = 0.67504436\n",
      "Iteration 57, loss = 0.52241166\n",
      "Iteration 15, loss = 0.96090453\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.1, solver=sgd; total time=  47.7s\n",
      "Iteration 16, loss = 0.76800982\n",
      "Iteration 1, loss = 1.14053372\n",
      "Iteration 24, loss = 0.68248212Iteration 23, loss = 0.62675691\n",
      "\n",
      "Iteration 2, loss = 1.05161050\n",
      "Iteration 58, loss = 0.51579019\n",
      "Iteration 16, loss = 0.94953751\n",
      "Iteration 17, loss = 0.79086819\n",
      "Iteration 2, loss = 1.13900664\n",
      "Iteration 25, loss = 0.70455900\n",
      "Iteration 24, loss = 0.63582640\n",
      "Iteration 3, loss = 1.07045041\n",
      "Iteration 59, loss = 0.51079145\n",
      "Iteration 17, loss = 1.04772438\n",
      "Iteration 18, loss = 0.79415734\n",
      "Iteration 3, loss = 1.10650959\n",
      "Iteration 4, loss = 1.08587007\n",
      "Iteration 26, loss = 0.64687415\n",
      "Iteration 25, loss = 0.67308595\n",
      "Iteration 60, loss = 0.50782784\n",
      "Iteration 18, loss = 0.88932919\n",
      "Iteration 19, loss = 0.79027213\n",
      "Iteration 4, loss = 1.02407791\n",
      "Iteration 27, loss = 0.60562039\n",
      "Iteration 5, loss = 1.07278478\n",
      "Iteration 26, loss = 0.63290425\n",
      "Iteration 61, loss = 0.50352644\n",
      "Iteration 19, loss = 0.92259142\n",
      "Iteration 20, loss = 0.78936170\n",
      "Iteration 5, loss = 0.95904317\n",
      "Iteration 28, loss = 0.64000849\n",
      "Iteration 6, loss = 1.05273736\n",
      "Iteration 62, loss = 0.51811701\n",
      "Iteration 27, loss = 0.64475894\n",
      "Iteration 20, loss = 0.89518981\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.75830547\n",
      "Iteration 29, loss = 0.64469153\n",
      "Iteration 6, loss = 0.96733689\n",
      "Iteration 7, loss = 0.97422280\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.1, solver=sgd; total time=  59.5s\n",
      "Iteration 63, loss = 0.53197783\n",
      "Iteration 28, loss = 0.59460092\n",
      "Iteration 22, loss = 0.74661588\n",
      "Iteration 7, loss = 1.03332879\n",
      "Iteration 30, loss = 0.63349472\n",
      "Iteration 8, loss = 0.96401563\n",
      "Iteration 64, loss = 0.50555550\n",
      "Iteration 29, loss = 0.56471978\n",
      "Iteration 23, loss = 0.73518307\n",
      "Iteration 8, loss = 0.97604211\n",
      "Iteration 31, loss = 0.63300356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 1.03672388\n",
      "Iteration 24, loss = 0.72016095\n",
      "Iteration 65, loss = 0.52771781\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.56151631\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.07525000000000001, solver=sgd; total time= 1.5min\n",
      "Iteration 9, loss = 1.00606600\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.0505, solver=sgd; total time= 3.0min\n",
      "Iteration 10, loss = 0.96986570\n",
      "Iteration 10, loss = 1.03971257\n",
      "Iteration 25, loss = 0.67846002\n",
      "Iteration 31, loss = 0.55818536\n",
      "Iteration 11, loss = 0.98145051\n",
      "Iteration 11, loss = 1.06293119\n",
      "Iteration 26, loss = 0.73403255\n",
      "Iteration 32, loss = 0.59933781\n",
      "Iteration 12, loss = 0.91158232\n",
      "Iteration 12, loss = 1.11494566\n",
      "Iteration 27, loss = 0.72221623\n",
      "Iteration 33, loss = 0.60871563\n",
      "Iteration 13, loss = 0.91669698\n",
      "Iteration 13, loss = 1.23333981\n",
      "Iteration 28, loss = 0.73314323\n",
      "Iteration 34, loss = 0.58729802\n",
      "Iteration 14, loss = 0.99959442\n",
      "Iteration 29, loss = 0.70237037\n",
      "Iteration 14, loss = 1.04598502\n",
      "Iteration 35, loss = 0.55380170\n",
      "Iteration 15, loss = 0.91827658\n",
      "Iteration 30, loss = 0.73431772\n",
      "Iteration 15, loss = 0.99897783\n",
      "Iteration 36, loss = 0.54975617\n",
      "Iteration 16, loss = 0.90624110\n",
      "Iteration 31, loss = 0.70793247\n",
      "Iteration 16, loss = 0.96034135\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.59897759\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.1, solver=sgd; total time=  32.7s\n",
      "Iteration 17, loss = 0.86324365\n",
      "Iteration 38, loss = 0.60529298\n",
      "Iteration 32, loss = 0.70314233\n",
      "Iteration 18, loss = 0.87410392\n",
      "Iteration 39, loss = 0.55478349\n",
      "Iteration 33, loss = 0.73921394\n",
      "Iteration 19, loss = 0.89510579\n",
      "Iteration 40, loss = 0.53186160\n",
      "Iteration 34, loss = 0.73522641\n",
      "Iteration 20, loss = 0.90952272\n",
      "Iteration 41, loss = 0.51445030\n",
      "Iteration 35, loss = 0.73014356\n",
      "Iteration 21, loss = 0.90492577\n",
      "Iteration 42, loss = 0.54513748\n",
      "Iteration 36, loss = 0.72569203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.1, solver=sgd; total time= 1.3min\n",
      "Iteration 22, loss = 0.94356071\n",
      "Iteration 43, loss = 0.51401381\n",
      "Iteration 23, loss = 0.95848986\n",
      "Iteration 44, loss = 0.50114087\n",
      "Iteration 24, loss = 0.88249293\n",
      "Iteration 45, loss = 0.50697621\n",
      "Iteration 25, loss = 0.86855983\n",
      "Iteration 46, loss = 0.54820573\n",
      "Iteration 26, loss = 0.86929288\n",
      "Iteration 47, loss = 0.54278185\n",
      "Iteration 27, loss = 0.84215218\n",
      "Iteration 48, loss = 0.54488324\n",
      "Iteration 28, loss = 0.79977481\n",
      "Iteration 49, loss = 0.51338246\n",
      "Iteration 29, loss = 0.76338227\n",
      "Iteration 50, loss = 0.50525851\n",
      "Iteration 30, loss = 0.76643661\n",
      "Iteration 51, loss = 0.50769834\n",
      "Iteration 31, loss = 0.76539378\n",
      "Iteration 52, loss = 0.54117515\n",
      "Iteration 32, loss = 0.73571970\n",
      "Iteration 53, loss = 0.54400221\n",
      "Iteration 33, loss = 0.71467861\n",
      "Iteration 54, loss = 0.50287178\n",
      "Iteration 34, loss = 0.71666829\n",
      "Iteration 55, loss = 0.51785671\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.07525000000000001, solver=sgd; total time= 1.9min\n",
      "Iteration 35, loss = 0.70390109\n",
      "Iteration 36, loss = 0.73455464\n",
      "Iteration 37, loss = 0.73369992\n",
      "Iteration 38, loss = 0.69802617\n",
      "Iteration 39, loss = 0.76542570\n",
      "Iteration 40, loss = 0.76044854\n",
      "Iteration 41, loss = 0.80447971\n",
      "Iteration 42, loss = 0.85260152\n",
      "Iteration 43, loss = 0.82855634\n",
      "Iteration 44, loss = 0.79670057\n",
      "Iteration 45, loss = 0.76307019\n",
      "Iteration 46, loss = 0.78882586\n",
      "Iteration 47, loss = 0.77671972\n",
      "Iteration 48, loss = 0.74544804\n",
      "Iteration 49, loss = 0.75777477\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, hidden_layer_sizes=77, learning_rate=constant, learning_rate_init=0.1, solver=sgd; total time= 1.1min\n",
      "Iteration 1, loss = 1.34351947\n",
      "Iteration 2, loss = 0.71488680\n",
      "Iteration 3, loss = 0.52946499\n",
      "Iteration 4, loss = 0.44203411\n",
      "Iteration 5, loss = 0.39121144\n",
      "Iteration 6, loss = 0.35828839\n",
      "Iteration 7, loss = 0.33070095\n",
      "Iteration 8, loss = 0.30932325\n",
      "Iteration 9, loss = 0.29336536\n",
      "Iteration 10, loss = 0.27954489\n",
      "Iteration 11, loss = 0.26737762\n",
      "Iteration 12, loss = 0.25677191\n",
      "Iteration 13, loss = 0.24851880\n",
      "Iteration 14, loss = 0.23910172\n",
      "Iteration 15, loss = 0.23206890\n",
      "Iteration 16, loss = 0.22620558\n",
      "Iteration 17, loss = 0.22003675\n",
      "Iteration 18, loss = 0.21325977\n",
      "Iteration 19, loss = 0.20974225\n",
      "Iteration 20, loss = 0.20366122\n",
      "Iteration 21, loss = 0.19945282\n",
      "Iteration 22, loss = 0.19586267\n",
      "Iteration 23, loss = 0.19059791\n",
      "Iteration 24, loss = 0.18602089\n",
      "Iteration 25, loss = 0.18383520\n",
      "Iteration 26, loss = 0.17992507\n",
      "Iteration 27, loss = 0.17844343\n",
      "Iteration 28, loss = 0.17482309\n",
      "Iteration 29, loss = 0.17316350\n",
      "Iteration 30, loss = 0.16882116\n",
      "Iteration 31, loss = 0.16580742\n",
      "Iteration 32, loss = 0.16378357\n",
      "Iteration 33, loss = 0.16047441\n",
      "Iteration 34, loss = 0.15883031\n",
      "Iteration 35, loss = 0.15634268\n",
      "Iteration 36, loss = 0.15484224\n",
      "Iteration 37, loss = 0.15351575\n",
      "Iteration 38, loss = 0.14964586\n",
      "Iteration 39, loss = 0.14876618\n",
      "Iteration 40, loss = 0.14682375\n",
      "Iteration 41, loss = 0.14355451\n",
      "Iteration 42, loss = 0.14160017\n",
      "Iteration 43, loss = 0.13894443\n",
      "Iteration 44, loss = 0.13788765\n",
      "Iteration 45, loss = 0.13643069\n",
      "Iteration 46, loss = 0.13569317\n",
      "Iteration 47, loss = 0.13309828\n",
      "Iteration 48, loss = 0.13160036\n",
      "Iteration 49, loss = 0.12921971\n",
      "Iteration 50, loss = 0.12836817\n",
      "Iteration 51, loss = 0.12638477\n",
      "Iteration 52, loss = 0.12556102\n",
      "Iteration 53, loss = 0.12497862\n",
      "Iteration 54, loss = 0.12252876\n",
      "Iteration 55, loss = 0.12126925\n",
      "Iteration 56, loss = 0.11994013\n",
      "Iteration 57, loss = 0.11846984\n",
      "Iteration 58, loss = 0.11724956\n",
      "Iteration 59, loss = 0.11539370\n",
      "Iteration 60, loss = 0.11359787\n",
      "Iteration 61, loss = 0.11291150\n",
      "Iteration 62, loss = 0.11150833\n",
      "Iteration 63, loss = 0.11053380\n",
      "Iteration 64, loss = 0.11048115\n",
      "Iteration 65, loss = 0.10899229\n",
      "Iteration 66, loss = 0.10828548\n",
      "Iteration 67, loss = 0.10574175\n",
      "Iteration 68, loss = 0.10418934\n",
      "Iteration 69, loss = 0.10333933\n",
      "Iteration 70, loss = 0.10282288\n",
      "Iteration 71, loss = 0.10083793\n",
      "Iteration 72, loss = 0.10081411\n",
      "Iteration 73, loss = 0.10094281\n",
      "Iteration 74, loss = 0.09833711\n",
      "Iteration 75, loss = 0.10113414\n",
      "Iteration 76, loss = 0.09893527\n",
      "Iteration 77, loss = 0.09609760\n",
      "Iteration 78, loss = 0.09725900\n",
      "Iteration 79, loss = 0.09462249\n",
      "Iteration 80, loss = 0.09443616\n",
      "Iteration 81, loss = 0.09219101\n",
      "Iteration 82, loss = 0.09217403\n",
      "Iteration 83, loss = 0.09105055\n",
      "Iteration 84, loss = 0.08999677\n",
      "Iteration 85, loss = 0.08944881\n",
      "Iteration 86, loss = 0.08833088\n",
      "Iteration 87, loss = 0.08812069\n",
      "Iteration 88, loss = 0.08657297\n",
      "Iteration 89, loss = 0.08525672\n",
      "Iteration 90, loss = 0.08552414\n",
      "Iteration 91, loss = 0.08427854\n",
      "Iteration 92, loss = 0.08309561\n",
      "Iteration 93, loss = 0.08505355\n",
      "Iteration 94, loss = 0.08268140\n",
      "Iteration 95, loss = 0.08235689\n",
      "Iteration 96, loss = 0.08136122\n",
      "Iteration 97, loss = 0.08125695\n",
      "Iteration 98, loss = 0.08118885\n",
      "Iteration 99, loss = 0.07815443\n",
      "Iteration 100, loss = 0.07806687\n",
      "Iteration 101, loss = 0.07751738\n",
      "Iteration 102, loss = 0.07741315\n",
      "Iteration 103, loss = 0.07573566\n",
      "Iteration 104, loss = 0.07620817\n",
      "Iteration 105, loss = 0.07327017\n",
      "Iteration 106, loss = 0.07319664\n",
      "Iteration 107, loss = 0.07234269\n",
      "Iteration 108, loss = 0.07261829\n",
      "Iteration 109, loss = 0.07234856\n",
      "Iteration 110, loss = 0.07339114\n",
      "Iteration 111, loss = 0.07201706\n",
      "Iteration 112, loss = 0.07111846\n",
      "Iteration 113, loss = 0.06973970\n",
      "Iteration 114, loss = 0.06947019\n",
      "Iteration 115, loss = 0.06797171\n",
      "Iteration 116, loss = 0.06812446\n",
      "Iteration 117, loss = 0.06651768\n",
      "Iteration 118, loss = 0.06716537\n",
      "Iteration 119, loss = 0.06651704\n",
      "Iteration 120, loss = 0.06606301\n",
      "Iteration 121, loss = 0.06557004\n",
      "Iteration 122, loss = 0.06504119\n",
      "Iteration 123, loss = 0.06429979\n",
      "Iteration 124, loss = 0.06392297\n",
      "Iteration 125, loss = 0.06400961\n",
      "Iteration 126, loss = 0.06399393\n",
      "Iteration 127, loss = 0.06290483\n",
      "Iteration 128, loss = 0.06232670\n",
      "Iteration 129, loss = 0.06111282\n",
      "Iteration 130, loss = 0.06016701\n",
      "Iteration 131, loss = 0.05961754\n",
      "Iteration 132, loss = 0.05920403\n",
      "Iteration 133, loss = 0.05908363\n",
      "Iteration 134, loss = 0.05911520\n",
      "Iteration 135, loss = 0.05913970\n",
      "Iteration 136, loss = 0.05730838\n",
      "Iteration 137, loss = 0.05692323\n",
      "Iteration 138, loss = 0.05717296\n",
      "Iteration 139, loss = 0.05667583\n",
      "Iteration 140, loss = 0.05575304\n",
      "Iteration 141, loss = 0.05538185\n",
      "Iteration 142, loss = 0.05533822\n",
      "Iteration 143, loss = 0.05472212\n",
      "Iteration 144, loss = 0.05478553\n",
      "Iteration 145, loss = 0.05418098\n",
      "Iteration 146, loss = 0.05306906\n",
      "Iteration 147, loss = 0.05341836\n",
      "Iteration 148, loss = 0.05311540\n",
      "Iteration 149, loss = 0.05285447\n",
      "Iteration 150, loss = 0.05234227\n",
      "Iteration 151, loss = 0.05161200\n",
      "Iteration 152, loss = 0.05138730\n",
      "Iteration 153, loss = 0.05198011\n",
      "Iteration 154, loss = 0.05068938\n",
      "Iteration 155, loss = 0.05072544\n",
      "Iteration 156, loss = 0.05024372\n",
      "Iteration 157, loss = 0.04982301\n",
      "Iteration 158, loss = 0.04896084\n",
      "Iteration 159, loss = 0.04910822\n",
      "Iteration 160, loss = 0.04932693\n",
      "Iteration 161, loss = 0.04885444\n",
      "Iteration 162, loss = 0.04834935\n",
      "Iteration 163, loss = 0.04802954\n",
      "Iteration 164, loss = 0.04844303\n",
      "Iteration 165, loss = 0.04772621\n",
      "Iteration 166, loss = 0.04745747\n",
      "Iteration 167, loss = 0.04691689\n",
      "Iteration 168, loss = 0.04695874\n",
      "Iteration 169, loss = 0.04645745\n",
      "Iteration 170, loss = 0.04619124\n",
      "Iteration 171, loss = 0.04608164\n",
      "Iteration 172, loss = 0.04597708\n",
      "Iteration 173, loss = 0.04573155\n",
      "Iteration 174, loss = 0.04545182\n",
      "Iteration 175, loss = 0.04520503\n",
      "Iteration 176, loss = 0.04487218\n",
      "Iteration 177, loss = 0.04444896\n",
      "Iteration 178, loss = 0.04473920\n",
      "Iteration 179, loss = 0.04449638\n",
      "Iteration 180, loss = 0.04413605\n",
      "Iteration 181, loss = 0.04385930\n",
      "Iteration 182, loss = 0.04388448\n",
      "Iteration 183, loss = 0.04393952\n",
      "Iteration 184, loss = 0.04378780\n",
      "Iteration 185, loss = 0.04326988\n",
      "Iteration 186, loss = 0.04293724\n",
      "Iteration 187, loss = 0.04290083\n",
      "Iteration 188, loss = 0.04268671\n",
      "Iteration 189, loss = 0.04269301\n",
      "Iteration 190, loss = 0.04262471\n",
      "Iteration 191, loss = 0.04233486\n",
      "Iteration 192, loss = 0.04212907\n",
      "Iteration 193, loss = 0.04199680\n",
      "Iteration 194, loss = 0.04193164\n",
      "Iteration 195, loss = 0.04179888\n",
      "Iteration 196, loss = 0.04165534\n",
      "Iteration 197, loss = 0.04170782\n",
      "Iteration 198, loss = 0.04153151\n",
      "Iteration 199, loss = 0.04143571\n",
      "Iteration 200, loss = 0.04132467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adriana/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from datetime import datetime\n",
    "ConvergenceWarning('ignore')\n",
    "\n",
    "\n",
    "def algorithm_pipeline(X_train_data, y_train_data, \n",
    "                       model, param_grid, cv=5, scoring_fit='accuracy'):\n",
    "    # What value should we use for scoring_fit, cv and activation?\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring_fit,\n",
    "        verbose=2,\n",
    "        return_train_score=True # set this for train score\n",
    "    )\n",
    "    return gs.fit(X_train_data, y_train_data)\n",
    "\n",
    "\n",
    "random_state = round(datetime.timestamp(datetime.now()))\n",
    "print(f'random_state = {random_state}')\n",
    "mlp = MLPClassifier(random_state=random_state, verbose=2)\n",
    "parameter_space = {\n",
    "    # 'hidden_layer_sizes' : np.linspace(10, 100, 5, dtype=int),\n",
    "    'hidden_layer_sizes' : np.linspace(50, 100, 5, dtype=int),\n",
    "    # 'hidden_layer_sizes' : [77],\n",
    "    'activation' : ['logistic'],\n",
    "    'solver' : ['sgd'],\n",
    "    'learning_rate_init' : np.linspace(0.001, 0.1, 9), #  Only used when solver=sgd or adam.\n",
    "    'learning_rate' : ['constant'] # Only used when solver='sgd'\n",
    "}\n",
    "clf = algorithm_pipeline(train_data, train_labels, mlp, parameter_space, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state = 1648668123\n",
      "MLPClassifier(activation='logistic', hidden_layer_sizes=77,\n",
      "              random_state=1648668123, solver='sgd', verbose=2)\n",
      "{'activation': 'logistic', 'hidden_layer_sizes': 77, 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'solver': 'sgd'}\n",
      "0.9492666666666667\n",
      "0.958\n"
     ]
    }
   ],
   "source": [
    "print(f'random_state = {random_state}')\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "print(clf.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result:\n",
    "  def __init__(self, hidden_layer_size, learning_rate, max_iter, accuracy):\n",
    "    self.hidden_layer_size = hidden_layer_size\n",
    "    self.learning_rate = learning_rate\n",
    "    self.max_iter = max_iter\n",
    "    self.accuracy = accuracy\n",
    "\n",
    "results = []\n",
    "for index in range(len(clf.cv_results_['param_hidden_layer_sizes'])):\n",
    "  result = Result(clf.cv_results_['param_hidden_layer_sizes'][index],\n",
    "                  clf.cv_results_['param_learning_rate_init'][index],\n",
    "                  # clf.cv_results_['param_max_iter'][index],\n",
    "                  0,\n",
    "                  clf.cv_results_['mean_test_score'][index])\n",
    "  results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adriana/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(clf.cv_results_['mean_train_score'], label='Train')\n",
    "plt.plot(clf.cv_results_['mean_test_score'], label='Test')\n",
    "plt.show()\n",
    "\n",
    "fig=plot_confusion_matrix(clf, test_data, test_labels, display_labels=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
    "fig.figure_.suptitle(\"Confusion Matrix for MNIST Test Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatex import Document, Command, LongTable\n",
    "from pylatex.utils import NoEscape\n",
    "\n",
    "doc = Document(page_numbers=False)\n",
    "\n",
    "# Add title\n",
    "doc.preamble.append(Command('title', 'Exercise 2b - MLP'))\n",
    "doc.preamble.append(Command('date', ''))\n",
    "doc.append(NoEscape(r'\\maketitle'))\n",
    "\n",
    "\n",
    "# Add table with results\n",
    "with doc.create(LongTable('l l l l l')) as data_table:\n",
    "  data_table.add_hline()\n",
    "  data_table.add_row(['Hidden Layer Size', 'Learning Rate', 'Max Iterations', 'Accuracy', 'Best?'])\n",
    "  data_table.add_hline()\n",
    "  data_table.end_table_header()\n",
    "  data_table.add_hline()\n",
    "\n",
    "  for result in results:\n",
    "    row = [result.hidden_layer_size,\n",
    "           round(result.learning_rate, 5),\n",
    "           result.max_iter,\n",
    "           round(result.accuracy, 5), \n",
    "           'YES' if round(clf.best_score_, 5) == round(result.accuracy, 5) else ''] \n",
    "\n",
    "    data_table.add_row(row)\n",
    "    data_table.add_hline()\n",
    "\n",
    "  data_table.end_table_last_footer()\n",
    "  \n",
    "\n",
    "try:\n",
    "  doc.generate_pdf('E2b')\n",
    "  doc.generate_tex('E2b')\n",
    "except:\n",
    "  # The generate_pdf sometimes fails and prints some message, but the pdf is actually generated. \n",
    "  # Not worth debugging :)\n",
    "  pass\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
