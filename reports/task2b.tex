\documentclass[10pt,a4paper,fullpage]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{array}
\usepackage{multirow}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{titling}

\geometry{a4paper, margin=1in}
\pagenumbering{gobble} 

\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\begin{document}

\title{Pattern Recognition - Exercise 2b (MLP)}
\author{}
\predate{}
\postdate{}
\date{\vspace{-5ex}}
\maketitle


%\textbf{Implementation details}
%\begin{itemize}
%	\item We didn't use a constant learning rate. Instead, the learning rate was decreasing with 10\% from one epoch to another.
%	\item We used cross validation with 5 number of folds to determine the pair (number of epochs, learning rate) for which we were able to train the model with the highest accuracy (on the validation set that we extracted from the initial train set). Then, with these 2 values found, we trained the model once again (on the whole train dataset this time) and evaluated it using the test dataset.
%	\item For computing the loss we used cross-entropy.
%	\item For the optimizer, we chose Stochastic Gradient Descent.
%\end{itemize}


\textbf{Testing details} 
\begin{itemize}
	\item 500 different configurations tested
	\item Number of iterations: 10, 32, 55, 77, 100
	\item Hidden layer size: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100
	\item Learning rate: 0.001, 0.012, 0.023, 0.034, 0.045, 0.067, 0..078, 0.089, 0.1
\end{itemize}

\textbf{Results}
\begin{itemize}
	\item Train accuracy: 98.39\%
	\item Test accuracy: 95.68\%
	\item Train loss: 0.064665
	\item Test loss: 0.144386
\end{itemize}

% From https://timodenk.com/blog/exporting-matplotlib-plots-to-latex/
\begin{figure}
	\begin{center}
%		\input{cnn_model_plot.pgf}
		\includegraphics[scale=1]{mlp_model.png}
	\end{center}
%	\caption{A PGF histogram from \texttt{matplotlib}.}
\end{figure}


\end{document}